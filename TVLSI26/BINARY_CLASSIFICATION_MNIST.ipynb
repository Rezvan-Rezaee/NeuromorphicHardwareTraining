{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac4b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TVLSI26.datsets.binary_mnist import BinaryMNIST\n",
    "from TVLSI26.configs.config import modelConstants, noiseRangeConfig\n",
    "from TVLSI26.neuron_models.digLIF import digLIF, Square\n",
    "from TVLSI26.ctt_weights.weight_variations import maskW, WeightDropoutLinear, apply_quant_noise, add_retention_noise\n",
    "from TVLSI26.models.populationEncodedBinaryClassifier import populationEncodedBinaryClassifier, _prediction_check, _population_code\n",
    "from TVLSI26.ctt_weights.noisePool import NoisePool, get_noise_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb3f2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bfb3f2e",
    "outputId": "6107ea54-98b6-495f-b4e4-df2a80a69581"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikegen\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ATY_PpTKtqot",
   "metadata": {
    "id": "ATY_PpTKtqot"
   },
   "outputs": [],
   "source": [
    "seed = int(time.time() * 1000) ^ random.getrandbits(32)\n",
    "counter_global = 0\n",
    "results_path = \"C:/Users/rezva/Documents/hardwareAwareLearning/results/\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e403dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "index0 = 0\n",
    "index1 = 8\n",
    "\n",
    "# or mnist_dataset = augmentedDataset(index0, index1, device, augmentation_type = \"HOG\", FULL_MNIST = False)\n",
    "mnist_dataset = BinaryMNIST(data_path=modelConstants.data_path, index0=index0, index1=index1, image_size=modelConstants.image_size)\n",
    "train_loader = DataLoader(mnist_dataset.get_train_data(), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_dataset.get_test_data(), batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "\n",
    "for dataset_name, dataset in zip([\"mnist_train\", \"mnist_test\"], (mnist_dataset.get_train_data(), mnist_dataset.get_test_data())):\n",
    "    print(f\"Number of images in {dataset_name}: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea05a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise pool for training\n",
    "noisePool = NoisePool()\n",
    "Ideal_Ids = noisePool.get_Ideal_Ids()\n",
    "\n",
    "pool_of_noise_dist = {\n",
    "    key: [val - key for val in values]\n",
    "    for key, values in noisePool.pool_of_weight_dist.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs=modelConstants.num_inputs):\n",
    "        super().__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "\n",
    "        self.fc1 = WeightDropoutLinear(self.num_inputs, modelConstants.num_outputs, bias=False)\n",
    "        self.lif1 = digLIF(beta=1, reset_mechanism=\"zero\")\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(\n",
    "        self,\n",
    "    ):\n",
    "        nn.init.normal_(\n",
    "            self.fc1.linear.weight,\n",
    "            mean=(modelConstants.max_weight - modelConstants.min_weight) / 2,\n",
    "            std=(modelConstants.max_weight - modelConstants.min_weight) / 16,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            self.fc1.linear.weight.clamp_(modelConstants.min_weight, modelConstants.max_weight)\n",
    "\n",
    "    def forward(self, x, noise_type=\"uniform\", tail_frac = 0.1):\n",
    "        step = 0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "\n",
    "        base_weights = self.fc1.linear.weight.clone()\n",
    "        quantized_weights = maskW(base_weights)  # clamp + quantize\n",
    "\n",
    "        noisy_weights_flat = []\n",
    "        for w in quantized_weights.view(-1):\n",
    "            closest_level = min(\n",
    "                pool_of_noise_dist.keys(), key=lambda l: abs(l - w.item())\n",
    "            )\n",
    "\n",
    "            if noise_type == \"mixed\":\n",
    "                mode = \"tail_high\" if closest_level < noisePool.ideal_Ids[6] else \"tail_low\"\n",
    "            else:\n",
    "                mode = noise_type\n",
    "\n",
    "            noise_sample = noisePool.sample_from_pool(\n",
    "                pool_of_noise_dist[closest_level],\n",
    "                mode = mode,\n",
    "                tail_frac=tail_frac\n",
    "            )\n",
    "                \n",
    "            noisy_weights_flat.append(w + noise_sample) #w + noise_sample\n",
    "            #print(len(noisy_weights_flat), noise_sample.dtype, w.dtype)\n",
    "\n",
    "        noisy_weights = torch.stack(noisy_weights_flat).reshape_as(quantized_weights)\n",
    "\n",
    "        if self.training and modelConstants.training_noise_ctt:\n",
    "            if modelConstants.retention_noise:\n",
    "                noisy_weights = add_retention_noise(\n",
    "                    self.fc1.linear.weight, modelConstants.std_ret_high, modelConstants.std_ret_low\n",
    "                )\n",
    "            if modelConstants.quantization_noise:\n",
    "                noisy_weights = apply_quant_noise(noisy_weights)\n",
    "        else:\n",
    "            noisy_weights = base_weights\n",
    "\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(x.shape[0]):\n",
    "            spk0 = x[step].flatten(1)\n",
    "            cur1 = Square.apply(x[step].flatten(1), noisy_weights)\n",
    "\n",
    "            noise1 = torch.randn_like(self.fc1.linear.weight) * modelConstants.std_ctt\n",
    "\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur1 = cur1 + nn.functional.linear(spk0, noise1)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            spk_rec.append(spk1)\n",
    "            mem_rec.append(mem1)\n",
    "\n",
    "        return torch.stack(spk_rec, dim=0), torch.stack(mem_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426807cc",
   "metadata": {
    "id": "426807cc"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "net = populationEncodedBinaryClassifier().to(device)\n",
    "net.fc1.linear.weight = nn.Parameter(torch.rand(net.fc1.linear.weight.shape).to(device) * modelConstants.max_weight)\n",
    "net.initialize_lif_thresholds(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=37e-4, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=.5)\n",
    "mse = nn.MSELoss()\n",
    "mseMem = nn.MSELoss()\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "mseCount = SF.mse_count_loss(population_code=True, num_classes=modelConstants.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2de194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = int(time.time() * 1000) ^ random.getrandbits(32)\n",
    "torch.manual_seed(seed)\n",
    "counter = 0\n",
    "step = 0\n",
    "lambda_std = 0.04\n",
    "lambda_min = 0.04\n",
    "commenting_on = False\n",
    "num_warmup_epochs = 4\n",
    "num_samples = 5\n",
    "\n",
    "def train_and_evaluate(net = net, fr_th = 0.5, loss_fn = mse, num_epochs = 5, population_code=True, optimizer = optimizer, \n",
    "                       scheduler=scheduler, train_loader = train_loader, test_loader = test_loader):\n",
    "\n",
    "    if population_code:\n",
    "        loss_fn = mseCount\n",
    "\n",
    "    best_weights = None\n",
    "    lowest_loss = 100.0\n",
    "    num_epochs += num_warmup_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_batch = iter(train_loader)\n",
    "        epoch_time = []\n",
    "        total_loss = 0\n",
    "        batch_idx = 0\n",
    "        lowest_b_acc = 1.0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            data = spikegen.latency(data, num_steps=modelConstants.num_steps, threshold=0.01, clip=True, first_spike_time=0, linear=True, normalize=True).cumsum(0)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = []\n",
    "            accuracies = []\n",
    "            loss_accum = 0\n",
    "\n",
    "            noise_type, tail_frac = get_noise_schedule(epoch)\n",
    "\n",
    "            if epoch < num_warmup_epochs:\n",
    "                training_noise_ctt = False\n",
    "            else:\n",
    "                training_noise_ctt = True\n",
    "                print(\"Training noise applied: \", noise_type, \", \", tail_frac)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                net.train()\n",
    "                spk_rec, mem_rec = net(data, noise_type=noise_type, tail_frac=tail_frac)\n",
    "\n",
    "                loss_sa = torch.zeros((1), dtype=dtype, device=device)\n",
    "                repeated_targets = (targets.float().unsqueeze(0).repeat(modelConstants.num_steps, 1).unsqueeze(-1))\n",
    "                fr = spk_rec.sum(0) / spk_rec.shape[0]\n",
    "\n",
    "                if(loss_fn == mse):\n",
    "                    fr = spk_rec.sum(0) / spk_rec.shape[0]\n",
    "                    loss_sa = loss_fn(fr.squeeze(), targets.float()) #mse\n",
    "                elif(loss_fn == bce):   \n",
    "                    loss_tensor = loss_fn(mem_rec - modelConstants.Threshold_voltage, repeated_targets)\n",
    "                    loss_sa = loss_tensor.sum()\n",
    "                elif(loss_fn == mseMem):\n",
    "                    repeated_targets *= modelConstants.Vdd\n",
    "                    loss_tensor = loss_fn(mem_rec, repeated_targets)\n",
    "                    loss_sa = loss_tensor.sum()\n",
    "                elif(loss_fn == mseCount):  \n",
    "                    loss_sa = mseCount._compute_loss(spk_rec, targets)\n",
    "\n",
    "                loss_accum += loss_sa\n",
    "\n",
    "                if population_code:\n",
    "                    _, _, num_outputs = _prediction_check(spk_rec)\n",
    "                    _, predicted_ = _population_code(spk_rec, modelConstants.num_classes, num_outputs).max(1)\n",
    "                else:\n",
    "                    fr = spk_rec.sum(0) / spk_rec.shape[0]  # Compute firing rate\n",
    "                    predicted_ = (fr.squeeze() > fr_th).int()\n",
    "\n",
    "                outputs.append(fr)\n",
    "                acc = (predicted_ == targets).sum().item() / targets.size(0)\n",
    "                if acc < lowest_b_acc:\n",
    "                    lowest_b_acc = acc\n",
    "                accuracies.append(acc)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            avg_acc = sum(accuracies) / len(accuracies)\n",
    "            stacked_outputs = torch.stack(outputs)\n",
    "\n",
    "            avg_pred = stacked_outputs.mean(dim=0) + 1e-8\n",
    "            safe_outputs = stacked_outputs + 1e-8\n",
    "            kl_div = (safe_outputs * (safe_outputs / avg_pred).log()).mean()\n",
    "\n",
    "            min_acc = min(accuracies)\n",
    "            min_acc_penalty = lambda_min * (1.0 - min_acc)\n",
    "\n",
    "            loss_val = (\n",
    "                (loss_accum / num_samples) + (lambda_std * kl_div) + min_acc_penalty\n",
    "            )\n",
    "            loss_val.backward()\n",
    "            #gradients = net.fc1.linear.weight.grad\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss_val.item()\n",
    "            end_time = time.time()\n",
    "            batch_idx += 1\n",
    "            with torch.no_grad():\n",
    "                stacked_out = torch.stack(outputs)\n",
    "                std_across_samples = stacked_out.std(dim=0).mean().item()\n",
    "                if commenting_on:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch+1} | Batch {batch_idx} | Loss: {loss_val.item():.4f} | Output std: {std_across_samples:.4f}\"\n",
    "                    )\n",
    "                print(\n",
    "                    f\"[E{epoch+1} B{batch_idx}] Avg acc: {avg_acc:.2f} | Min acc: {min(accuracies):.2f} | Max acc: {max(accuracies):.2f}\"\n",
    "                )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                net.fc1.linear.weight.clamp_(0, modelConstants.max_weight)\n",
    "\n",
    "        if commenting_on:\n",
    "            print(f\"epoch time {end_time - start_time:.2f}s\")\n",
    "            print(\"epoch loss: \", loss_val)\n",
    "\n",
    "        epoch_time.append(end_time - start_time)\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            for data, targets in test_loader:\n",
    "                data = data.to(device)\n",
    "                data = spikegen.latency(data, num_steps=modelConstants.num_steps, threshold=0.01, clip=True, first_spike_time=0, linear=True, normalize=True,).cumsum(0)\n",
    "                targets = targets.to(device)\n",
    "                spk_rec, _ = net(data)\n",
    "\n",
    "                if population_code:\n",
    "                    correct += SF.accuracy_rate(spk_rec, targets, population_code=population_code, num_classes=num_classes, ) * spk_rec.size(1)\n",
    "                else:\n",
    "                    fr = spk_rec.sum(0) / spk_rec.shape[0]\n",
    "                    predicted = (fr.squeeze() > fr_th).int()\n",
    "                    correct += (predicted == targets).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "            if commenting_on:\n",
    "                print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "                print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\\n\")\n",
    "            for param_group in optimizer.param_groups:\n",
    "                current_lr = param_group[\"lr\"]\n",
    "                if commenting_on:\n",
    "                    print(f\"Current learning rate: {current_lr}\")\n",
    "        if (total_loss / len(train_loader)) < lowest_loss:\n",
    "            lowest_loss = total_loss / len(train_loader)\n",
    "            best_weights = net.fc1.linear.weight.clone()\n",
    "        print(\n",
    "            f\"\\nEpoch {epoch+1} Summary: Avg Loss = {total_loss / len(train_loader):.4f}\\n\"\n",
    "        )\n",
    "    print(\"average epoch time: \", np.mean(epoch_time))\n",
    "    net.fc1.linear.weight.data.copy_(best_weights)\n",
    "    return 100 * (correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e58691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_binary_classifier(net=net, test_loader=test_loader, fr_th=0.4):\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for data, targets in test_loader:\n",
    "            data = data.to(device)\n",
    "            data = spikegen.latency(data, num_steps=num_steps, threshold=0.01, clip=True, first_spike_time=0, linear=True, normalize=True).cumsum(0)\n",
    "            targets = targets.to(device)\n",
    "            spk_rec, _ = net(data)\n",
    "\n",
    "            if modelConstants.population_code:\n",
    "                correct += SF.accuracy_rate(spk_rec, targets, population_code=modelConstants.population_code, num_classes=modelConstants.num_classes) * spk_rec.size(1)\n",
    "            else:\n",
    "                fr = spk_rec.sum(0) / spk_rec.shape[0]\n",
    "                predicted = (fr.squeeze() > fr_th).int()\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "        total += targets.size(0)\n",
    "\n",
    "        print(f\"Total correctly classified test set images with noisy weights: {correct}/{total}\")\n",
    "        print(f\"Test Set Accuracy with noisy weights: {100 * correct / total:.2f}%\\n\")\n",
    "        return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a987f078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "best_accuracy = 0\n",
    "best_params = None\n",
    "worst_accuracy = 100 \n",
    "\n",
    "thr_values = [0.3]\n",
    "fr_th_values = [0.5]\n",
    "\n",
    "hyperparams = [{\"thr\": thr, \"fr_th\": fr_th} for thr, fr_th in product(thr_values, fr_th_values)]\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2)\n",
    "\n",
    "for params in hyperparams:\n",
    "    fr_th = params[\"fr_th\"]\n",
    "    V_th = params[\"thr\"]\n",
    "    net.initialize_lif_thresholds(device, V_th=V_th)\n",
    "    accuracy = train_and_evaluate(fr_th=fr_th, loss_fn=mseMem, num_epochs = 20, population_code=modelConstants.population_code, optimizer = optimizer, scheduler=scheduler, train_loader = train_loader, test_loader = test_loader)\n",
    "    print(f\"Accuracy with params {params}: {accuracy}\")\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "        best_weights = net.fc1.linear.weight.clone()\n",
    "    if accuracy < worst_accuracy:\n",
    "        worst_accuracy = accuracy\n",
    "        worst_params = params\n",
    "        \n",
    "    print(f\"Best parameters: {best_params} with accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85f437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 80 # minimum samples to start convergence check\n",
    "max_samples = 200 # to prevent infinite loop\n",
    "threshold = 0.001  # acceptable relative change (0.1%)\n",
    "accuracies = []\n",
    "\n",
    "previous_mean = None\n",
    "previous_std = None\n",
    "sample_idx = 0\n",
    "\n",
    "while True:\n",
    "    sample_idx += 1\n",
    "\n",
    "    for i in range(4):\n",
    "        model = copy.deepcopy(net)\n",
    "        perturbed_state_dict = model.state_dict()\n",
    "\n",
    "        for name, param in perturbed_state_dict.items():\n",
    "            if 'weight' in name:\n",
    "                with torch.no_grad():\n",
    "                    perturbed_weights = []\n",
    "                    for w in param.view(-1).cpu().numpy():\n",
    "                        closest_level = min(Ideal_Ids, key=lambda x: abs(x - w))\n",
    "                        if i==0:\n",
    "                            noisy_value = np.random.choice(noisePool.pool_of_weight_dist[closest_level])\n",
    "                        elif i==1:\n",
    "                            noisy_value = noisePool.sample_from_pool(noisePool.pool_of_weight_dist[closest_level], mode=\"uniform\", original_weight=w)\n",
    "                        elif i==2:\n",
    "                            noisy_value = noisePool.sample_from_pool(noisePool.pool_of_weight_dist[closest_level], mode=\"uniform\", original_weight=w)\n",
    "                        elif i==3:\n",
    "                            if closest_level < Ideal_Ids[6]:\n",
    "                                noisy_value = noisePool.sample_from_pool(noisePool.pool_of_weight_dist[closest_level], mode=\"tail_high\", tail_frac=0.001, p_static=0.0, original_weight=w)\n",
    "                            else:\n",
    "                                noisy_value = noisePool.sample_from_pool(noisePool.pool_of_weight_dist[closest_level], mode=\"tail_low\", tail_frac=0.001, p_static=0.0, original_weight=w)\n",
    "                        perturbed_weights.append(noisy_value)\n",
    "                    param.copy_(torch.tensor(np.array(perturbed_weights).reshape(param.shape), dtype=param.dtype))\n",
    "\n",
    "        model.load_state_dict(perturbed_state_dict)\n",
    "        acc = evaluate_binary_classifier(net=model)\n",
    "        accuracies.append(acc)\n",
    "        print(f\"Sample {sample_idx}: Accuracy = {acc:.2f}%, Mean = {np.mean(accuracies):.2f}%, Std = {np.std(accuracies):.2f}%\")\n",
    "    \n",
    "    if sample_idx >= min_samples:\n",
    "        current_mean = np.mean(accuracies)\n",
    "        current_std = np.std(accuracies)\n",
    "\n",
    "        if previous_mean is not None and previous_std is not None:\n",
    "            mean_change = abs(current_mean - previous_mean) / previous_mean\n",
    "            std_change = abs(current_std - previous_std) / previous_std\n",
    "\n",
    "            if mean_change < threshold and std_change < threshold:\n",
    "                print(f\"Converged after {sample_idx} samples.\")\n",
    "                break\n",
    "\n",
    "        previous_mean = current_mean\n",
    "        previous_std = current_std\n",
    "\n",
    "    if sample_idx >= max_samples:\n",
    "        print(f\"Reached maximum samples ({max_samples}) without full convergence.\")\n",
    "        break\n",
    "\n",
    "print(f\"Mean accuracy: {np.mean(accuracies):.2f}%\")\n",
    "print(f\"Best-case accuracy: {np.max(accuracies):.2f}%\")\n",
    "print(f\"Worst-case accuracy: {np.min(accuracies):.2f}%\")\n",
    "print(f\"Std: {np.std(accuracies):.2f}%\")\n",
    "\n",
    "plt.hist(accuracies, bins=10)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Monte Carlo Sampling Accuracy Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62239d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"minimum = \", np.min(accuracies))\n",
    "print(\"maximum = \", np.max(accuracies))\n",
    "print(\"mean = \", np.mean(accuracies))\n",
    "print(\"std = \", np.std(accuracies))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab5d03",
   "metadata": {},
   "source": [
    "### Generating a CSV file with the quantized CTT weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8dbb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on hardware simulations and CTT-supported weight levels \n",
    "weights_L = [0.020000, 0.05327272727272728, 0.08654545454545455, 0.11981818181818182, \n",
    "             0.15309090909090908, 0.18636363636363637, 0.21963636363636363, 0.2529090909090909, \n",
    "             0.2861818181818182, 0.3194545454545455, 0.3527272727272728, 0.386000]\n",
    "threshold_values = [110, 75, 55, 51, 43, 35, 23, 18, 14, 10, 7, 0]\n",
    "def quantize_value(w):\n",
    "    if (w>=modelConstants.max_weight):\n",
    "      return modelConstants.max_weight\n",
    "    elif (w<=modelConstants.min_weight):\n",
    "      return modelConstants.min_weight\n",
    "    else:\n",
    "      step = (modelConstants.max_weight - modelConstants.min_weight) / (modelConstants.num_w_levels - 1)\n",
    "      return round((w - modelConstants.min_weight) / step) * step + modelConstants.min_weight\n",
    "weights_np = []\n",
    "weights_np.append(0)\n",
    "replacement_index = 0\n",
    "first_neuron = False\n",
    "if first_neuron:\n",
    "   max_index = 100\n",
    "   min_index = 0\n",
    "else:\n",
    "   max_index = 200\n",
    "   min_index = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbee067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weights = best_weights\n",
    "if(modelConstants.pretrained_weights_used):\n",
    "    for i in range(min_index, min(max_index, trained_weights.view(-1).numel())):\n",
    "        if first_neuron:\n",
    "            j = i + 1\n",
    "        else:\n",
    "            j= i + 1 - 100\n",
    "        weight = trained_weights.view(-1)[i].item()\n",
    "        weight = quantize_value(weight)\n",
    "        replacement_index = weights_L.index(weight)\n",
    "        print(f\"V{j}={threshold_values[replacement_index]}m \",end='')\n",
    "        weights_np.append(threshold_values[replacement_index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not modelConstants.pretrained_weights_used):\n",
    "    for name, param in net.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                for i in range(min_index, min(max_index, param.numel())):\n",
    "                    if first_neuron:\n",
    "                         j = i + 1\n",
    "                    else:\n",
    "                        j= i + 1 - 100\n",
    "                    weight = param.view(-1)[i].item()\n",
    "                    weight = quantize_value(weight)\n",
    "                    replacement_index = weights_L.index(weight)\n",
    "                    print(f\"V{j}={threshold_values[replacement_index]}m \",end='')\n",
    "                    weights_np.append(threshold_values[replacement_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KldUFzcU4emc",
   "metadata": {
    "id": "KldUFzcU4emc"
   },
   "source": [
    "### Generating OCEAN script for Cadence input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardwareImage(pix):\n",
    "    modifiedPixel = 336 - pix*20\n",
    "    return modifiedPixel\n",
    "\n",
    "def softwareImage(modifiedPixel):\n",
    "    pix = (336 - modifiedPixel) / 20\n",
    "    return pix\n",
    "\n",
    "num_data_points = len(mnist_dataset.get_test_data())\n",
    "test_loader_h = DataLoader(mnist_dataset.get_test_data(), batch_size=num_data_points, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GDBcN8EL4qLE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDBcN8EL4qLE",
    "outputId": "1e8e1b38-5da2-47b2-81e7-4577365187c0"
   },
   "outputs": [],
   "source": [
    "fr_np = []\n",
    "\n",
    "for data, targets in test_loader_h:\n",
    "    data = data.to(device)\n",
    "    data_preSNN = data\n",
    "    data = spikegen.latency(data, num_steps=modelConstants.num_steps, threshold=0.01, clip=True, first_spike_time=0, linear=True, normalize=True).cumsum(0)\n",
    "\n",
    "    if(modelConstants.pretrained_weights_used):\n",
    "        with torch.no_grad():  # Disable gradient tracking\n",
    "            net.fc1.linear.weight.data.copy_(trained_weights)\n",
    "\n",
    "    spk_rec, mem = net(data)\n",
    "    #fr = (spk_rec.sum(0) / spk_rec.shape[0])\n",
    "    fr = spk_rec.sum(0)\n",
    "    summed_data = torch.sum(data, axis=0)\n",
    "print(data.shape)\n",
    "print(summed_data.shape, num_data_points)\n",
    "\n",
    "hardwareInput = hardwareImage(summed_data)\n",
    "\n",
    "inputData = np.zeros((num_data_points, modelConstants.num_inputs))\n",
    "\n",
    "for k in range(num_data_points):\n",
    "    temp_image = hardwareInput[k]\n",
    "    flat_image = temp_image.view(-1)\n",
    "\n",
    "    for pixel in range (flat_image.numel()):\n",
    "        inputData[k][pixel] = flat_image[pixel]\n",
    "\n",
    "test_data = inputData.T\n",
    "\n",
    "filename_inData = f'{results_path}/inputData{num_data_points}39ideal_v{counter_global}.csv'\n",
    "with open(filename_inData,'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(test_data)\n",
    "\n",
    "for elem in fr.view(-1):\n",
    "    fr_np.append(elem.detach().cpu().item())\n",
    "\n",
    "firing_rate = np.array(fr_np).reshape(-1, 1)\n",
    "\n",
    "filename = f'{results_path}/firing_rate{num_data_points}39ideal_v{counter_global}.csv'\n",
    "with open(filename,'w') as h:\n",
    "    csv.writer(h).writerows(firing_rate)\n",
    "counter_global += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(edgeitems=torch.numel(fr), threshold=torch.numel(fr))\n",
    "fr[0:num_data_points, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b423c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "current_datetime = datetime.now()\n",
    "datetime_string = current_datetime.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "hardware_filename = \"bc_\" + datetime_string\n",
    "hardware_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z4zO-7Z75R92",
   "metadata": {
    "id": "z4zO-7Z75R92"
   },
   "outputs": [],
   "source": [
    "# generating text file for a batch of test inputs\n",
    "filename = f'{results_path}/sim_{num_data_points}_v{counter_global}.txt'\n",
    "fout = open(filename, \"wt\")\n",
    "fout.write(\"ii = 0 \\n\")\n",
    "\n",
    "for k in range(num_data_points):\n",
    "    fout.write('''simulator( 'spectre )\n",
    "design( \"DigNeuron_New\" \"EntireNeuronArchV2SingleCell_Rezvan\" \"schematic\")\n",
    "resultsDir( \"/data/shared/brainer/home/zmoham12/simulation/DigNeuron_New/EntireNeuronArchV2SingleCell_Rezvan/maestro/results/maestro/.tmpADEDir_zahra.mohammadrezaee/DigNeuron_New_EntireNeuronArchV2SingleCell_1/simulation/EntireNeuronArchV2SingleCell_Rezvan/spectre/schematic\" )\n",
    "modelFile( \n",
    "    '(\"$SPECTRE_MODEL_PATH/design_wrapper.lib.scs\" \"tt_pre\")\n",
    ")\n",
    "analysis('tran ?stop \"340n\"  )\n",
    "desVar(\t  \"VG\" 110m\t)\n",
    "desVar(\t  \"VD\" 0.8\t)\n",
    "desVar(\t  \"f\" 20n\t)\n",
    "''')\n",
    "    temp_image = hardwareInput[k]\n",
    "    flat_image = temp_image.view(-1)\n",
    "\n",
    "    for pixel in range (flat_image.numel()):\n",
    "        A = 'desVar(       \"Vd{}\" {}n )'\n",
    "        fout.write(A.format(pixel,flat_image[pixel]) + \"\\n\")\n",
    "\n",
    "    fout.write(f'''\n",
    "desVar(\t  \"wx\" 0m\t)\n",
    "envOption(\n",
    "\t'analysisOrder  list(\"pz\" \"dcmatch\" \"stb\" \"tran\" \"envlp\" \"ac\" \"dc\" \"lf\" \"noise\" \"xf\" \"sp\" \"pss\" \"pac\" \"pstb\" \"pnoise\" \"pxf\" \"psp\" \"qpss\" \"qpac\" \"qpnoise\" \"qpxf\" \"qpsp\" \"hb\" \"hbac\" \"hbstb\" \"hbnoise\" \"hbxf\" \"sens\" \"acmatch\")\n",
    "\t'switchViewList  list(\"spectre\" \"spectreText\" \"cmos_sch\" \"cmos.sch\" \"schematic\" \"veriloga\")\n",
    ")\n",
    "option(\t'pzSeverity  \"None\"\n",
    "\t'noiseSeverity  \"None\"\n",
    "\t'spSeverity  \"None\"\n",
    "\t'acSeverity  \"None\"\n",
    "\t'dcOpSeverity  \"None\"\n",
    "\t'dcSeverity  \"None\"\n",
    "\t'tranSeverity  \"None\"\n",
    ")\n",
    "temp( 27 )\n",
    "out = outfile(\"results{hardware_filename}.txt\" \"a\")\n",
    "run()\n",
    "selectResult( 'tran )\n",
    "outputs()\n",
    "ocnPrint(?output out sample( VT(\"/Fire\") 0 340n \"linear\" 1n) )\n",
    "close(out)\n",
    "for( i 1 30  printf( \"%d \\\\n\" ii ))\n",
    "ii = ii+1\n",
    "''')\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac61776",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.genfromtxt('C:/Users/rezva/Documents/hardwareAwareLearning/results/inputData2019_v2.csv', delimiter=',', dtype=np.float32)\n",
    "inputData = test_data.T\n",
    "total_elements = inputData.size\n",
    "num_data_points = total_elements // (modelConstants.image_size * modelConstants.image_size)\n",
    "\n",
    "if total_elements % (modelConstants.image_size * modelConstants.image_size) != 0:\n",
    "    raise ValueError(\"Input data cannot be reshaped into the desired dimensions. Check the CSV file format.\")\n",
    "hardwareInput_reconstructed = torch.tensor(inputData).view(num_data_points, 1, modelConstants.image_size * modelConstants.image_size)\n",
    "print(\"Reconstructed tensor shape:\", hardwareInput_reconstructed.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f804d6",
   "metadata": {},
   "source": [
    "### Reading Cadence Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9e18f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input text file path and output CSV file path\n",
    "input_file = r'C:/Users/rezva/Documents/hardwareAwareLearning/results/resultsideal_39_2025_01_13_20_54_22_.txt'  # Replace with your text file path\n",
    "output_file = r'C:/Users/rezva/Documents/hardwareAwareLearning/results/CadenceResults.csv'\n",
    "\n",
    "# Read the text file and save it as a CSV\n",
    "# Adjust delimiter as needed based on the text file's structure (default: space-separated)\n",
    "try:\n",
    "    # Read the text file into a DataFrame\n",
    "    df = pd.read_csv(input_file, delimiter=r'\\s+', engine=\"python\")\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"File has been successfully converted and saved as {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9a0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = pd.read_csv(\"C:/Users/rezva/Documents/hardwareAwareLearning/results/CadenceResults.csv\")\n",
    "data1 = np.asarray(data0.iloc[:, 1])\n",
    "num_data_points = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce15bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_sim = np.zeros([num_data_points,])\n",
    "\n",
    "stay = True\n",
    "rising_edge = True\n",
    "rise_voltage = 0.35\n",
    "fall_voltage = 0.3\n",
    "data_index = 0\n",
    "\n",
    "csv_file_path = \"C:/Users/rezva/Documents/hardwareAwareLearning/results/processed_cadence.csv\"\n",
    "if(modelConstants.generate_processed_data):\n",
    "  with open(csv_file_path, mode=\"w\", newline=\"\") as file:\n",
    "      writer = csv.writer(file)\n",
    "      writer.writerow([\"(s)\"])\n",
    "\n",
    "for prediction_index in range(num_data_points):\n",
    "  while stay:\n",
    "    # Extracting values one by one from excell\n",
    "    temp = str(data1[data_index])\n",
    "    prefix = temp[-1]\n",
    "\n",
    "    # Checking if we have a new input\n",
    "    if temp == '(s)':\n",
    "      #print(\"done\")\n",
    "      rising_edge = True\n",
    "      if(modelConstants.generate_processed_data):\n",
    "        with open(csv_file_path, mode=\"a\", newline=\"\") as file:\n",
    "          writer = csv.writer(file)\n",
    "          writer.writerow([\"(s)\"]) \n",
    "      data_index += 1\n",
    "      break\n",
    "\n",
    "    # Checking to see if there are any prefixes and replacing them with the proper value\n",
    "    if prefix == 'm':\n",
    "      temp = temp[:-1]\n",
    "      temp = float(temp)\n",
    "      temp *= 1e-3\n",
    "    elif prefix == 'u':\n",
    "      temp = temp[:-1]\n",
    "      temp = float(temp)\n",
    "      temp *= 1e-6\n",
    "    elif prefix == 'nano':\n",
    "      temp = temp[:-1]\n",
    "      temp = float(temp)\n",
    "      temp *= 1e-9\n",
    "    elif prefix == 'p':\n",
    "      temp = temp[:-1]\n",
    "      temp = float(temp)\n",
    "      temp *= 1e-12\n",
    "    else :\n",
    "      temp = 0\n",
    "\n",
    "    if(modelConstants.generate_processed_data):\n",
    "      with open(csv_file_path, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([temp]) \n",
    "\n",
    "    # Checking if there was a spike\n",
    "    if (temp > rise_voltage) and not rising_edge:\n",
    "      rising_edge = True\n",
    "      y_test_predicted_sim[prediction_index] += 1\n",
    "\n",
    "    if (temp < fall_voltage) and rising_edge:\n",
    "      rising_edge = False\n",
    "      \n",
    "    if data_index < (data1.size - 1):\n",
    "      data_index += 1\n",
    "    else:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c025185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max value: \", y_test_predicted_sim.max(), \"min value: \", y_test_predicted_sim.min())\n",
    "np.set_printoptions(edgeitems=np.size(y_test_predicted_sim), threshold=np.size(y_test_predicted_sim))\n",
    "y_test_predicted_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_software_results = np.array([ 9.,  8.,  9., 11., 11.,  6.,  8.,  9., 11., 11.,  6.,  6.,  4.,  8.,\n",
    "         9.,  5.,  8.,  5., 10.,  7.,  8.,  9.,  6., 12., 12., 11., 12.,  6.,\n",
    "         5.,  8.,  9., 10., 10., 10., 11.,  5.,  8.,  8.,  6.,  6.,  9., 10.,\n",
    "         8.,  8.,  9.,  7.,  8.,  8.,  7.,  4., 13., 10.,  4.,  5.,  5.,  9.,\n",
    "         9., 10., 12.,  5.,  9., 10., 11., 11., 11.,  9.,  6., 13.,  6., 11.,\n",
    "         9.,  7., 12., 14.,  6., 10., 10., 12.,  6.,  6., 12.,  9., 11., 12.,\n",
    "        11.,  9.,  9., 10.,  8.,  7., 11., 12.,  4.,  6., 10.,  9., 11.,  5.,\n",
    "         6.,  3.,  8.,  7.,  6., 10.,  9.,  8., 13., 11., 13., 12., 15.,  4.,\n",
    "        11.,  4., 10., 12., 10.,  5.,  5.,  8.,  8.,  6., 10.,  9.,  8., 11.,\n",
    "         5.,  6.,  7., 10.,  6., 11.,  5.,  9.,  9., 10., 12.,  9.,  8.,  7.,\n",
    "         9.,  7.,  7.,  6.,  9.,  9., 11.,  4., 12.,  9., 13.,  8.,  4., 11.,\n",
    "         8., 11.,  5., 12.,  9.,  3.,  6., 12.,  9., 10.,  7.,  9.,  8.,  8.,\n",
    "        12.,  7., 11., 12.,  7.,  9.,  8.,  9.,  5., 10., 10.,  9.,  6.,  3.,\n",
    "         3.,  8., 10.,  4.,  3., 10., 11.,  5., 12., 13.,  9.,  4.,  8.,  7.,\n",
    "         6.,  6.,  8.,  8.,  6., 11.,  8., 11.,  5.,  6., 10.,  6.,  5.,  8.,\n",
    "        12.,  9.,  5.,  9.,  9., 11.,  3., 11.,  4.,  7.,  7., 14.,  7.,  4.,\n",
    "         9.,  7.,  7.,  8.,  3., 10., 12.,  6., 10., 11.,  9., 10.,  5.,  9.,\n",
    "         4.,  8.,  6.,  5.,  6., 10.,  6., 10., 13.,  7.,  8.,  5.,  4.,  7.,\n",
    "         3.,  8.,  7.,  5., 10., 12.,  6.,  6., 12.,  8.,  6.,  5.,  7.,  9.,\n",
    "         9., 13.,  3.,  5.,  9.,  4.,  6.,  8., 12.,  4.,  9.,  9.,  8., 11.,\n",
    "        11.,  6., 13.,  6.,  8.,  4., 12.,  8., 11.,  4.,  6., 10.,  6., 10.,\n",
    "        10.,  7.,  9.,  6., 11.,  9.,  6.,  4.,  9., 10.,  6., 13., 11., 11.,\n",
    "         8.,  7.,  5.,  5.,  5.,  4.,  5.,  5., 12.,  8.,  6., 10.,  6., 11.,\n",
    "         7.,  7., 11.,  4., 10.,  3.,  7., 10.,  7., 10., 10.,  7., 11.,  8.,\n",
    "         9., 11., 14., 10.,  6.,  4., 10.,  3.,  9., 11., 11.,  6.,  8.,  8.,\n",
    "        10., 10.,  5.,  8.,  9., 11., 11.,  8.,  3., 10., 11.,  8., 10., 10.,\n",
    "         8., 11.,  5.,  7.,  4., 10.,  4.,  8., 10.,  7.,  7.,  6., 10.,  6.,\n",
    "         8.,  9.,  8.,  5.,  8.,  4., 13.,  3., 12.,  4., 10., 11.,  5., 10.,\n",
    "         9., 12.,  5.,  9., 12.,  9.,  6., 11.,  9., 10.,  6.,  4.,  6.,  9.,\n",
    "         6.,  5., 10., 11.,  9., 11.,  6.,  5.,  8., 12., 11.,  3.,  6.,  9.,\n",
    "        11.,  7., 12., 10.,  8.,  7.,  6.,  3.,  5.,  6.,  9., 10.,  9.,  9.,\n",
    "         6.,  8.,  6.,  4., 15., 11., 12., 11.,  6., 11.,  6., 10.,  8., 14.,\n",
    "         6.,  4.,  7.,  8., 10.,  7., 10., 11.,  9.,  9.,  4.,  6.,  9.,  7.,\n",
    "         8.,  6.,  6.,  8.,  5.,  9.,  7., 11.,  9.,  7.,  5., 13., 10.,  4.,\n",
    "         7.,  9.,  7.,  9.,  5.,  4., 12.,  7., 10., 11.,  8., 11., 10.,  7.,\n",
    "         7.,  5.,  6., 10., 10.,  5.,  5.,  9.,  4.,  4.,  8.,  9.,  7.,  6.,\n",
    "         5.,  9.,  6.,  9.,  9., 10.,  9.,  3.,  6.,  8., 11.,  9.,  9.,  9.,\n",
    "         7., 12.,  4., 14.,  4., 14.,  6.,  7., 12., 12.,  6.,  8.,  9.,  6.,\n",
    "        11.,  9.,  4.,  8.,  7.,  8.,  4.,  8.,  9., 11.,  3.,  8.,  5.,  5.,\n",
    "        12.,  9., 10., 11.,  4., 10.,  9.,  8.,  7.,  6., 11.,  5.,  7.,  6.,\n",
    "        10., 13., 14.,  9., 11.,  6., 10., 10.,  6.,  8., 12.,  6., 11., 10.,\n",
    "        13.,  9.,  3.,  7.,  9., 10., 10.,  9.,  3., 10., 10., 11., 11.,  5.,\n",
    "        11., 10., 10., 10., 10.,  6.,  7.,  8., 11.,  3.,  9.,  9.,  8.,  4.,\n",
    "        10.,  9., 12., 12.,  9.,  8., 10.,  9., 10.,  5.,  9., 12.,  7.,  6.,\n",
    "        12., 14.,  7., 10., 10.,  4., 11.,  6.,  6.,  7., 10., 11.,  9.,  9.,\n",
    "        12.,  9.,  9.,  7., 12.,  8., 12.,  7., 10.,  8.,  9., 10.,  6., 11.,\n",
    "         9., 15.,  6.,  6.,  9., 11., 11.,  4.,  8., 12., 14., 11.,  5.,  9.,\n",
    "        12.,  7.,  7.,  5., 13., 11., 10., 12.,  7., 12., 11.,  7., 10.,  7.,\n",
    "         9.,  6.,  4., 15., 10.,  7., 12., 12., 12.,  6.,  4.,  9.,  5.,  4.,\n",
    "         7.,  7.,  4.,  8.,  5.,  8.,  5.,  5.,  5.,  5., 11.,  5., 10.,  5.,\n",
    "        10.,  7.,  6., 10.,  5.,  7., 11.,  7.,  7.,  5.,  4., 12.,  9., 12.,\n",
    "         7., 11.,  9., 10.,  8.,  6.,  7.,  9.,  5., 10., 10., 10.,  5.,  9.,\n",
    "         5.,  9.,  9.,  4., 11., 10.,  9.,  6., 10.,  7.,  6.,  7.,  6.,  6.,\n",
    "         8., 11., 10., 11.,  9.,  8.,  5., 10., 12.,  6.,  7., 12.,  7.,  7.,\n",
    "        12.,  8.,  7.,  8.,  8., 11.,  5.,  8., 10., 11., 12.,  9.,  7., 11.,\n",
    "        11., 10.,  6.,  5., 12.,  8.,  6.,  8., 10.,  7.,  8., 10.,  8., 10.,\n",
    "         8.,  5.,  8.,  9.,  8.,  8.,  8.,  8., 10.,  7.,  8., 12.,  5., 10.,\n",
    "        12.,  4.,  4.,  7., 15.,  6., 12., 12.,  9.,  5.,  6.,  9.,  6.,  4.,\n",
    "         5., 11.,  9.,  5.,  4.,  4.,  7., 11.,  7.,  6.,  6.,  8.,  6.,  9.,\n",
    "        10.,  9.,  5.,  9.,  6., 13., 10.,  5.,  6.,  9., 13.,  9., 11.,  4.,\n",
    "         8.,  8., 10.,  5., 12., 11., 11., 11.,  9.,  6., 12., 10.,  8.,  7.,\n",
    "        13.,  6., 12., 11.,  8., 14.,  8., 11.,  7.,  7.,  5.,  7.,  8.,  7.,\n",
    "         7.,  9.,  9.,  4.,  5., 13., 14., 14., 11., 12.,  9.,  8., 11., 12.,\n",
    "         8.,  6.,  9.,  5.,  9.,  9., 11., 11.,  4., 10., 11.,  8., 12.,  7.,\n",
    "         5.,  4.,  8., 10.,  9.,  6.,  9.,  7., 11.,  5.,  7., 15.,  5.,  9.,\n",
    "         5., 13.,  8., 11.,  6., 11.,  5.,  8.,  6.,  5.,  8.,  7.,  8.,  8.,\n",
    "        15.,  8., 13.,  9., 13., 10.,  4., 10.,  5., 10.,  8.,  6., 10.,  2.,\n",
    "         7.,  6., 15., 10.,  5.,  8., 12.,  8., 11.,  6.,  8., 12.,  4.,  5.,\n",
    "         3., 10., 13.,  6.,  5., 11., 10.,  4., 12.,  5.,  9.,  4., 11.,  6.,\n",
    "         9.,  5.,  9., 14., 13., 10.,  9.,  9., 11., 11., 10.,  8.,  8.,  7.,\n",
    "        10., 12.,  7.,  4., 11., 10., 12.,  7.,  8.,  6.,  7., 13.,  5.,  8.,\n",
    "         6.,  5.,  6., 12., 11.,  6.,  8.,  9.,  8., 10.,  9., 11.,  8.,  4.,\n",
    "        10., 12.,  8.,  6.,  6.,  7., 10.,  9.,  7., 11., 11., 11.,  6., 12.,\n",
    "         8.,  8.,  8.,  9.,  8., 12., 11.,  6.,  8.,  4.,  4., 11.,  6., 10.,\n",
    "        10.,  7., 12.,  9.,  5., 11., 14.,  6.,  5.,  9., 10., 14.,  6.,  5.,\n",
    "        10.,  7.,  7.,  5., 13.,  6., 10., 12., 10.,  8.,  8.,  7.,  8., 13.,\n",
    "         7.,  6., 11.,  9.,  9., 13.,  7.,  7.,  8.,  5.,  7.,  4.,  9.,  7.,\n",
    "        11., 10., 13., 12., 12., 12.,  4.,  9., 10.,  6., 13.,  9., 12., 10.,\n",
    "        10., 13., 12.,  7.,  8.,  7.,  7., 10.,  7.,  9., 11.,  7.,  5.,  5.,\n",
    "         9.,  6.,  5.,  8.,  6.,  5.,  9.,  3.,  8.,  6.,  5.,  9., 11.,  7.,\n",
    "        11.,  8.,  8., 11., 11., 13.,  9., 10.,  6.,  8.,  6.,  8.,  6.,  7.,\n",
    "        14., 10.,  9.,  6.,  7., 10.,  5.,  8., 13.,  9.,  4.,  5., 10.,  5.,\n",
    "        11.,  7., 12.,  8.,  6.,  4., 11.,  5.,  5., 10., 13.,  7.,  7.,  6.,\n",
    "         4.,  9., 12.,  8.,  9.,  8.,  8.,  8., 14.,  9., 11., 10., 13.,  7.,\n",
    "         9.,  6., 11.,  8., 12., 11.,  6.,  6.,  7., 11., 10., 11.,  9.,  5.,\n",
    "         4.,  4., 11.,  9., 10., 13., 11.,  5., 12.,  4.,  7., 10.,  5.,  6.,\n",
    "         9., 11.,  6., 11.,  8.,  7.,  9.,  8.,  6., 11.,  8., 10., 12.,  7.,\n",
    "         6., 10.,  9., 10.,  7.,  7.,  7., 10., 12.,  8., 10.,  8.,  8., 12.,\n",
    "         6.,  8., 12., 11., 11., 12., 10.,  5.,  5.,  4., 11., 10.,  8., 13.,\n",
    "        10., 10.,  9.,  3.,  7.,  9., 10., 10.,  6.,  8.,  5., 11.,  6.,  6.,\n",
    "         4.,  9.,  3.,  3.,  9.,  8.,  8.,  9.,  5.,  5.,  5.,  8.,  5.,  6.,\n",
    "        11.,  8., 10., 10.,  7.,  7.,  4.,  5., 10.,  6.,  8.,  9.,  5.,  4.,\n",
    "         3., 10.,  6., 11.,  4.,  6.,  7.,  4., 10.,  7.,  6.,  9., 15., 11.,\n",
    "         8., 10.,  8.,  8.,  7., 11.,  9.,  9., 11.,  7., 12., 10., 12.,  8.,\n",
    "         5.,  9.,  5., 10., 11., 12.,  9., 10.,  7., 11.,  5.,  5., 13., 15.,\n",
    "        10., 11.,  5.,  6.,  4.,  8.,  4., 11.,  6., 11., 14., 11., 11.,  9.,\n",
    "         9.,  7.,  7.,  7., 11.,  8., 11.,  6.,  9., 11.,  9., 11., 11.,  7.,\n",
    "        10.,  7.,  9., 12.,  6.,  4.,  5.,  4.,  9.,  4., 10., 10.,  4., 12.,\n",
    "        12., 12., 11.,  5.,  9.,  3., 10., 10.,  5.,  9.,  7., 13.,  5., 13.,\n",
    "        11.,  7.,  6.,  9., 10.,  3.,  9.,  7., 13.,  8.,  4., 11.,  7.,  4.,\n",
    "        13., 10., 10., 11.,  9.,  5.,  5., 10.,  6., 11.,  3.,  7.,  4., 12.,\n",
    "         7.,  9., 10., 11., 10., 13., 10.,  5.,  5., 13.,  7.,  5.,  5.,  6.,\n",
    "         8.,  6.,  8.,  9.,  9.,  4.,  4.,  7.,  4.,  8., 11.,  7.,  9., 10.,\n",
    "         5.,  8.,  8., 12., 11.,  5.,  4., 10., 10.,  8., 12., 11., 12.,  8.,\n",
    "        11., 13., 12.,  7.,  6., 12.,  7.,  4.,  5.,  7., 10., 10., 13.,  8.,\n",
    "         9.,  8., 12.,  4., 10., 11.,  8.,  6.,  8., 10., 10.,  9., 10.,  6.,\n",
    "         6., 10.,  7., 12.,  6.,  5.,  4., 14.,  5.,  8., 14.,  7.,  7.,  5.,\n",
    "         9., 11.,  5.,  9.,  5.,  4.,  6.,  5.,  9.,  7.,  9., 12.,  5.,  7.,\n",
    "         6., 10.,  5., 10.,  4.,  8., 13.,  7., 11., 11., 12.,  5., 11.,  9.,\n",
    "        10.,  6.,  6.,  5.,  6.,  5., 10.,  9., 10., 10., 11.,  7.,  4.,  5.,\n",
    "         6., 13., 10.,  5.,  5., 13.,  5.,  6.,  7.,  5.,  9.,  5.,  7., 10.,\n",
    "         7.,  8., 10.,  8., 12., 10., 10., 12.,  7.,  8., 10., 10.,  6.,  7.,\n",
    "        12.,  5.,  7.,  9.,  6.,  7.,  6., 10.,  8., 13., 10., 10., 13.,  8.,\n",
    "         8.,  5.,  9.,  8., 11., 10., 12., 14.,  4., 10.,  6.,  5., 10.,  8.,\n",
    "        12.,  9.,  9., 10., 12.,  9., 11.,  8.,  9.,  4.,  5.,  8.,  6., 10.,\n",
    "        11., 11.,  6.,  9.,  7., 10.,  6.,  9.,  6.,  8.,  8., 11.,  9.,  7.,\n",
    "        10., 10.,  4., 11.,  4.,  7.,  7.,  9., 12.,  6.,  4., 12.,  9., 11.,\n",
    "         6., 10.,  7.,  5., 10., 14.,  8.,  6., 12., 11.,  9.,  7.,  9.,  8.,\n",
    "        11.,  7.,  7.,  7.,  8., 11., 10.,  5., 10., 14.,  5., 10.,  5., 10.,\n",
    "         8.,  5.,  7.,  7., 11., 11., 11., 10., 11.,  6., 12.,  4.,  8.,  4.,\n",
    "        11.,  8.,  8.,  7., 11.,  7.,  8.,  8.,  8.,  9.,  8., 10., 11., 11.,\n",
    "        10., 10., 12.,  5.,  8., 11.,  4.,  6.,  4., 10., 11., 12.,  6., 11.,\n",
    "         8.,  9.,  4.,  9.,  8.,  8.,  6.,  6.,  8.,  2., 14.,  7.,  5.,  8.,\n",
    "         8.,  8.,  7., 10., 11.,  5.,  6.,  9., 11.,  9., 12.,  7.,  7.,  4.,\n",
    "         8.,  6.,  5.,  5.,  6.,  9.,  7., 11.,  8., 11., 10., 11.,  4., 10.,\n",
    "        11.,  8.,  7.,  4., 11., 12.,  5., 11., 11.,  6.,  5.,  9.,  4.,  8.,\n",
    "         9.,  6., 12., 12.,  9.,  8.,  6.,  5.,  8., 12.,  8., 12.,  5., 11.,\n",
    "        12., 12.,  5.,  4.,  5.,  9.,  9., 10.,  8., 10.,  8.,  9.,  8., 12.,\n",
    "         8.,  6.,  7., 10., 11.,  9., 13.,  6.,  5., 13., 11.,  9.,  9.,  7.,\n",
    "         7.,  7.,  7.,  9.,  4., 10., 12.,  9.,  6., 10.,  9.,  8.,  9., 11.,\n",
    "        12.,  7.,  8.,  7., 11., 10.,  4., 11.,  9., 11., 11., 10., 11., 11.,\n",
    "         6., 10.,  7., 10., 12., 11., 11.,  4.,  7.,  7., 13.,  2.,  4., 11.,\n",
    "         4.,  9.,  9.,  8.,  9.,  9.,  4., 10.,  8., 11.,  8.,  6.,  9.,  8.,\n",
    "         6., 14., 10.,  8., 10., 11.,  7.,  6.,  5.,  8.,  4.,  7., 12.,  9.,\n",
    "        11.,  7., 10.,  5.,  8.,  3.,  5.,  7.,  4., 10.,  7.,  9.,  8., 10.,\n",
    "         8.,  9.,  3.,  8., 10.,  9.,  9.,  9.,  7., 10.,  7.,  7., 10.,  8.,\n",
    "         9.,  9., 12.,  3.,  9.,  9.,  7., 11., 13.,  9., 11., 10.,  4.,  8.,\n",
    "        10., 10., 10.,  5.,  7.,  6., 10., 10.,  4.,  4.,  8., 11.,  3., 11.,\n",
    "         8.,  8., 10.,  9.,  5.,  3., 12., 10.,  7.,  6.,  7.,  8.,  9., 11.,\n",
    "         6., 11.,  8., 10., 12.,  7.,  9.,  7.,  9.,  6., 13., 11.,  6., 13.,\n",
    "         7.,  7.,  9., 14., 10.,  8.,  7., 11., 11., 11.,  6.,  9.,  7.,  9.,\n",
    "         9.,  7., 11.,  5.,  6.,  8.,  9.,  7.,  8.,  8.,  8., 10., 10.,  8.,\n",
    "         6., 12.,  8.,  9.,  7.,  9.,  9.,  8.,  2.,  4.,  6.,  7., 13., 13.,\n",
    "         7.,  4.,  7.,  9., 11.,  4.,  8.,  9.,  9., 11.,  9.,  4.,  8., 11.,\n",
    "         4.,  7.,  9.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"max value: \", fr_software_results.max(), \"min value: \", fr_software_results.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23202f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between corresponding elements\n",
    "difference = fr_software_results - y_test_predicted_sim\n",
    "\n",
    "# Calculate the error percentage\n",
    "error_percentage = np.abs(difference)\n",
    "\n",
    "# Print the error percentage array\n",
    "print(\"max err: \", error_percentage.max(), \"min err: \", error_percentage.min(), \"average err\", np.mean(error_percentage))\n",
    "print(\"Error percentage:\", error_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e0f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy considering both neurons\n",
    "if modelConstants.num_outputs % modelConstants.num_classes != 0:\n",
    "    raise ValueError(\n",
    "        f\"`num_outputs` ({modelConstants.num_outputs}) must be a multiple of `num_classes` ({modelConstants.num_classes}).\"\n",
    "    )\n",
    "\n",
    "fr_0 = torch.tensor([11.,  9.,  7.,  7.,  7.,  8.,  7., 12.,  5.,  8.,  7.,  9.,  7.,\n",
    "        5., 12.,  8.,  6.,  7.,  9.,  8.,  3.,  9., 11.,  5.,  7.,  6.,\n",
    "        5., 10.,  9.,  3.,  4.,  5.,  5.,  5.,  5., 10., 11.,  5.,  8.,\n",
    "        7.,  4.,  4.,  5.,  4., 10., 10.,  9., 10., 10.,  8.,  7.,  8.,\n",
    "        7.,  9., 11., 13.,  5.,  6.,  5.,  6.,  5.,  5.,  4.,  6.,  9.,\n",
    "       10., 12., 12.,  3.,  4.,  5.,  5., 10.,  7., 10.,  5.,  9.,  7.,\n",
    "        9., 12.,  9.,  9., 13.,  7.,  6.,  5.,  5., 11.,  9., 12.,  6.,\n",
    "        8.,  7.,  5.,  5.,  5., 11., 10.,  8.,  7., 11.,  4., 10.,  5.,\n",
    "       10., 11.,  6.,  5.,  6.,  6., 11.,  8.,  6.,  8.,  5.,  5.,  9.,\n",
    "        3., 10.,  4., 11.,  7.,  5.,  4.,  4.,  5.,  4.,  9., 13., 12.,\n",
    "       12.,  5., 10.,  6.,  5.,  5.,  8.,  6.,  6.,  4.,  3.,  3.,  5.,\n",
    "       11., 10.,  9.,  5.,  7.,  4.,  4.,  5.,  9.,  9., 12.,  4., 11.,\n",
    "        9.,  8.,  5.,  7.,  7.,  6.,  4.,  5., 10., 10.,  4.,  8.,  7.,\n",
    "        4.,  6.,  6., 11.,  8.,  3.,  4., 11.,  4.,  5., 10.,  5.,  8.,\n",
    "        7., 10.,  5.,  7.,  7.,  5.,  4.,  7.,  4.,  6.,  6.,  9.,  4.,\n",
    "        8.,  7., 10., 12.,  5., 11.,  4.,  4.,  8., 10.,  9., 10.,  8.,\n",
    "        8., 11.,  5.,  9., 11., 10., 12., 10.,  6.,  4.,  9., 11., 13.,\n",
    "        9., 12.,  8.,  6.,  4., 10.,  9.,  7.,  6.,  6.,  6., 11.,  5.,\n",
    "        4.,  5.,  8., 11.,  8.,  4.,  4.,  6.,  7.,  4., 10.,  6., 12.,\n",
    "        4.,  6., 11.,  8.,  9.,  7.,  4., 12.,  9.,  4.,  5.,  9., 11.,\n",
    "        6.,  4.,  9.,  7., 11.,  9.,  5., 13.,  7.,  7., 11.,  5., 13.,\n",
    "       11.,  4., 10.,  4., 13.,  3.,  4., 11., 12.,  8.,  9., 12.,  9.,\n",
    "        7.,  4.,  4.,  8.,  9.,  5., 12.,  5.,  4., 10.,  4., 12.,  5.,\n",
    "        6., 10.,  8.,  5.,  5., 12.,  8.,  6.,  6.,  4.,  4.,  7.,  8.,\n",
    "       12.,  4.,  8., 10.,  6.,  4., 10.,  6., 11.,  5.,  5., 10.,  9.,\n",
    "        9.,  6.,  2.,  4.,  5., 10., 13.,  6., 12.,  7.,  4.,  4.,  5.,\n",
    "        8.,  5.,  8., 11., 11.,  7.,  6.,  5.,  7., 10.,  8.,  4.,  6.,\n",
    "        5.,  6., 10., 12.,  5.,  6.,  4.,  4., 10.,  6.,  4.,  4.,  5.,\n",
    "        4.,  5., 11.,  9.,  9.,  5.,  9.,  5.,  6.,  8.,  8.,  9.,  5.,\n",
    "        4., 11.,  4.,  4.,  8.,  5.,  7.,  5.,  6.,  6.,  9.,  5.,  6.,\n",
    "        9.,  8.,  3.,  6., 10.,  7.,  8., 10.,  8.,  6.,  5., 11., 11.,\n",
    "        5.,  8.,  5.,  9.,  5.,  9.,  5.,  4.,  5.,  7.,  5.,  4.,  5.,\n",
    "        4.,  4.,  9.,  4.,  4.,  9.,  5.,  4.,  9., 10., 12.,  5.,  7.,\n",
    "        3., 10.,  5.,  4.,  9.,  2., 11.,  7.,  8., 12.,  6.,  6.,  5.,\n",
    "       11.,  6.,  8.,  5., 10., 12., 11.,  7., 10.,  4.,  5.,  4.,  8.,\n",
    "        3.,  5.,  5.,  6.,  3.,  4., 12., 13.,  7.,  8., 11.,  8.,  4.,\n",
    "        9.,  9., 10.,  9.,  7.,  6.,  6.,  6.,  5.,  6., 10., 10., 10.,\n",
    "        8.,  6., 10.,  5., 12., 12., 13.,  4., 11.,  4., 11.,  9.,  5.,\n",
    "        4., 10.,  3.,  5.,  8.,  7., 11.,  9., 10., 10.,  9.,  3.,  9.,\n",
    "        7.,  7., 10.,  7.,  5.,  7., 11.,  7.,  4.,  5., 11., 10.,  7.,\n",
    "        8., 14.,  8.,  6.,  3.,  4.,  6.,  7., 12.,  9.,  4.,  9.,  4.,\n",
    "        5.,  8.,  4., 12., 11.,  8.,  8.,  8.,  6.,  7.,  5.,  4.,  8.,\n",
    "        5.,  5., 13.,  5.,  8.,  5., 13., 12.,  5., 13.,  4., 11.,  8.,\n",
    "       12.,  4.,  8.,  7., 14.,  6.,  7.,  5.,  6.,  8.,  8.,  6., 10.,\n",
    "        5.,  6.,  7.,  3.,  6., 11.,  6.,  7.,  5.,  4.,  6.,  5.,  5.,\n",
    "       12.,  7.,  6.,  5.,  5.,  4., 12.,  6., 12., 11.,  9., 12.,  5.,\n",
    "       11.,  9.,  4., 10.,  6., 11.,  9., 13., 12.,  4.,  6., 11., 10.,\n",
    "       12., 10.,  6., 10., 11.,  6.,  7.,  4.,  9.,  5.,  3.,  4., 11.,\n",
    "        9., 11.,  5.,  7.,  9.,  3., 10.,  4., 10.,  3.,  6., 11.,  9.,\n",
    "       11.,  9.,  4.,  5.,  5.,  9.,  5.,  5., 12.,  9., 10.,  5.,  8.,\n",
    "        5.,  4., 10.,  6., 11.,  6.,  9.,  4.,  8.,  7.,  4., 10.,  5.,\n",
    "        8.,  5.,  9., 10.,  6.,  4., 11., 10.,  9., 12.,  3.,  9.,  7.,\n",
    "       11., 10.,  8.,  5.,  6.,  7.,  9., 14., 12.,  7.,  9.,  3., 10.,\n",
    "       11., 11.,  6., 10., 10.,  8., 11.,  6.,  7.,  5., 10.,  5.,  9.,\n",
    "        9., 12.,  9., 10.,  6., 11.,  9.,  9.,  8., 12.,  4.,  6.,  4.,\n",
    "        5.,  4.,  6.,  4.,  9.,  9., 10.,  9.,  5.,  4., 12.,  9.,  5.,\n",
    "        8.,  5.,  4.,  5.,  4., 10., 10.,  8., 12.,  5.,  7.,  9.,  8.,\n",
    "        7.,  9.,  7., 11.,  6.,  5., 11.,  7.,  8.,  5.,  7.,  8.,  9.,\n",
    "        8., 11.,  9.,  5.,  5.,  5.,  6.,  5., 12., 11.,  5., 11., 11.,\n",
    "       12., 12.,  6., 11.,  4.,  9., 12.,  6., 10.,  5.,  4., 10.,  6.,\n",
    "        5.,  5.,  5.,  6.,  5.,  9.,  8.,  6.,  5., 11., 11.,  4., 11.,\n",
    "       11.,  7.,  6., 10.,  5.,  6.,  5., 10., 13.,  8., 12.,  7.,  7.,\n",
    "        4.,  6., 11.,  4.,  8.,  6., 12.,  6.,  6., 10.,  9., 10.,  3.,\n",
    "       10., 11., 13., 10., 11.,  7.,  4.,  5.,  5.,  8.,  5.,  4.,  6.,\n",
    "        5.,  7., 11.,  5.,  7.,  4.,  6.,  9.,  3.,  4.,  5.,  9.,  7.,\n",
    "        9.,  5.,  5.,  4.,  3.,  5.,  8.,  3.,  7.,  7.,  3.,  6.,  5.,\n",
    "        4.,  7.,  4.,  9., 10.,  3.,  7., 11.,  4.,  4.,  4.,  5.,  8.,\n",
    "        9., 10.,  6.,  6.,  6.,  4.,  7., 12., 14.,  4.,  6.,  4.,  8.,\n",
    "        4.,  9.,  7., 11.,  5.,  6.,  9.,  7., 12., 11.,  5., 11.,  8.,\n",
    "        8.,  4.,  4., 10.,  8.,  6.,  6.,  5.,  7.,  7.,  8.,  8., 11.,\n",
    "        6.,  7., 10.,  7.,  4.,  5.,  6.,  7.,  8.,  9.,  6., 10.,  5.,\n",
    "       11.,  9., 13.,  9.,  4.,  8., 10., 10.,  4.,  7., 13.,  3.,  9.,\n",
    "        7.,  6., 12.,  4.,  8.,  6., 12., 13.,  8.,  5.,  6.,  7.,  4.,\n",
    "        6.,  8., 10.,  9., 13.,  5.,  9.,  8.,  5.,  9.,  9.,  6.,  9.,\n",
    "       11.,  7.,  5., 10., 12., 10.,  4.,  7.,  6.,  4.,  4., 12.,  6.,\n",
    "        5.,  6., 12., 11.,  9.,  8.,  7.,  9.,  9.,  5.,  5.,  8.,  4.,\n",
    "       10., 10., 10., 12.,  8., 10.,  8., 11.,  5.,  6.,  6.,  8.,  4.,\n",
    "        6.,  5.,  5.,  9.,  5., 14., 11.,  6.,  5.,  9., 12.,  9., 10.,\n",
    "       11.,  4.,  6.,  5.,  6.,  4., 11.,  6.,  4.,  4., 11.,  4., 11.,\n",
    "        6.,  5., 10.,  9.,  6., 10.,  6., 11.,  8., 11., 10.,  8., 11.,\n",
    "        9.,  9.,  9.,  8., 12.,  7., 12., 12.,  7.,  8.,  4.,  7.,  9.,\n",
    "        8.,  9.,  9., 10.,  6.,  5.,  5., 12., 10.,  4.,  7., 10.,  8.,\n",
    "        4.,  4.,  9., 11., 12.,  9.,  4., 10., 13.,  9.,  8.,  4.,  4.,\n",
    "       13.,  6.,  5.,  6.,  6.,  8., 13., 10.,  9.,  9.,  6.,  5.,  4.,\n",
    "        4.,  7., 12.,  4.,  3., 14., 11., 11., 11.,  4., 11., 10., 10.,\n",
    "       10., 10.,  7., 10.,  4.,  3., 12.,  5.,  7., 10., 11.,  8.,  5.,\n",
    "       11., 10.,  3.,  4., 11.,  7., 14.,  6., 11., 10.,  9., 11., 10.,\n",
    "        8., 10., 10.,  6.,  4.,  4., 12., 11.,  4.,  9.,  3.,  7.,  5.,\n",
    "        7.,  4.,  4.,  9.,  6.,  9.,  5.,  4.,  4.,  9.,  6., 10.,  9.,\n",
    "        6.,  6.,  5., 12.,  9.,  9.,  4.,  6.,  3.,  5.,  4.,  4.,  8.,\n",
    "        7.,  4.,  5.,  5.,  8.,  4.,  8., 10., 15.,  5.,  6.,  6., 10.,\n",
    "        5.,  6., 12.,  5., 11.,  4.,  8.,  7.,  9.,  7.,  4.,  5.,  5.,\n",
    "        5., 10.,  5.,  9.,  8.,  5., 10., 11.,  5.,  5., 13.,  5., 12.,\n",
    "        9.,  5.,  5., 12.,  6.,  7., 13., 10.,  7., 10.,  6.,  7.,  5.,\n",
    "        8., 12.,  8., 10.,  7., 10., 12., 12.,  7.,  6.,  9.,  9.,  6.,\n",
    "        5.,  5.,  6.,  4., 10.,  8., 10.,  5.,  7., 14.,  6.,  6.,  5.,\n",
    "        8.,  7.,  4., 10.,  5.,  5.,  2.,  7., 11.,  6.,  8., 11.,  5.,\n",
    "        5.,  9.,  6.,  5.,  4.,  3.,  5., 10.,  9.,  8.,  5.,  8., 11.,\n",
    "        5.,  4.,  8.,  4.,  8., 12.,  8., 11.,  6., 10., 10.,  4.,  5.,\n",
    "        8.,  6.,  4.,  9.,  4.,  8., 10., 10., 10., 15., 11., 10.,  9.,\n",
    "       12.,  7.,  5., 12., 12.,  7., 13.,  5., 11., 12.,  7.,  4.,  5.,\n",
    "        4.,  6.,  9., 11.,  4.,  9.,  4.,  6.,  6.,  9.,  5.,  6.,  6.,\n",
    "        8.,  8.,  7.,  8.,  5.,  4.,  8.,  9., 10.,  4.,  7.,  6., 10.,\n",
    "        6.,  7., 13.,  4.,  5.,  8.,  4.,  7.,  7.,  6.,  4.,  5.,  8.,\n",
    "        8.,  6., 13.,  6.,  4.,  3.,  5.,  9.,  5.,  9., 11.,  9.,  6.,\n",
    "        9.,  4.,  8.,  5.,  8.,  8., 14.,  6.,  4.,  6.,  9.,  6.,  5.,\n",
    "       13.,  5., 10., 10.,  3., 12., 10.,  6.,  5.,  2.,  8.,  5.,  5.,\n",
    "        7., 12.,  3., 14., 11.,  9.,  6., 14.,  9.,  6., 13.,  5.,  6.,\n",
    "        5., 10., 10.,  9., 10.,  6.,  8.,  3., 10.,  6.,  9.,  9.,  5.,\n",
    "        5.,  6.,  6.,  8.,  7.,  6.,  4.,  4., 11.,  4.,  4., 10.,  4.,\n",
    "        4.,  8.,  5.,  9., 10., 10.,  8.,  6.,  5.,  8., 10.,  4.,  7.,\n",
    "       14.,  4.,  8., 10.,  7., 10.,  4.,  8., 14.,  4.,  9.,  6.,  8.,\n",
    "        5.,  7.,  9.,  9., 12.,  6., 11.,  8.,  7.,  9.,  6.,  4.,  5.,\n",
    "        9.,  9., 13.,  7.,  9.,  8., 11., 11.,  4.,  9.,  6.,  5., 11.,\n",
    "        6., 13., 10.,  5.,  4.,  5.,  7.,  5.,  7.,  8.,  8.,  3.,  9.,\n",
    "        4.,  9.,  9.,  4.,  4.,  5.,  5.,  9.,  9.,  8.,  9.,  5.,  8.,\n",
    "        7.,  5., 12.,  8., 10., 10.,  9.,  5.,  8., 11.,  9.,  9.,  5.,\n",
    "        4.,  6.,  8.,  7.,  4.,  4., 10., 10., 11.,  8.,  7.,  9.,  6.,\n",
    "        4.,  5.,  8.,  9.,  9.,  8., 10.,  5., 11., 10.,  8.,  7.,  7.,\n",
    "       10.,  7.,  9., 10.,  8.,  4.,  5.,  8.,  9., 12., 13.,  4.,  4.,\n",
    "       12.,  9.,  3., 11.,  6.,  4.,  5.,  9., 13.,  5.,  3.,  6., 10.,\n",
    "        8.,  7.,  4., 14.,  6.,  5.,  5.,  6.,  7., 10.,  8.,  5., 11.,\n",
    "        7.,  5.,  8.,  6.,  9.,  7.,  9., 10.,  8., 10.,  5.,  5.,  4.,\n",
    "        6.,  7., 10.,  5.,  8., 11.,  9.,  6.,  5.,  4.,  6.,  7.,  7.,\n",
    "       11., 11.,  5.,  3.,  8.,  4.,  9.,  4.,  9.,  6.,  4., 10.,  4.,\n",
    "        5., 10.,  4.,  9., 12.,  9.,  6.,  9.,  8.,  7.,  6.,  5.,  8.,\n",
    "        9.,  5., 11., 11., 12.,  8.,  5., 12., 10., 11.,  4.,  8., 11.,\n",
    "        4.,  5.,  5.,  8.,  6.,  4.,  7.,  5.,  9.,  5., 12.,  7.,  5.,\n",
    "        5.,  5., 12.,  9.,  8.,  9.,  4.,  5.,  5.,  7.,  5.,  6.,  6.,\n",
    "        7.,  6., 10.,  5.,  6.,  4.,  5.,  7.,  8.,  9., 10.,  4.,  5.,\n",
    "       13.,  5.,  5., 11., 13., 11.,  9.,  8., 11.,  5., 11.,  9.,  7.,\n",
    "        7.,  5.,  4., 11.,  5.,  4.,  4.,  7.,  4.,  5.,  4.,  9.,  7.,\n",
    "        4.,  7., 10.,  4., 12., 11.,  8.,  6.,  3., 10., 12.,  7.,  7.,\n",
    "        5.,  6.,  5.,  4., 11.,  7.,  9.,  4., 12., 10.,  7., 11., 12.,\n",
    "        8., 11., 10., 13., 12.,  5.,  7.,  6.,  4., 11.,  4.,  8., 10.,\n",
    "        5.,  8.,  6.,  5.,  4.,  7.,  4.,  3., 11.,  9.,  9.,  7.,  5.,\n",
    "        5.,  5.,  6., 12., 10.,  5., 10., 12.,  7.,  7.,  7.,  6.,  9.,\n",
    "        6., 11.,  9.,  5.,  7.,  4.,  8.,  6.,  7., 12.,  6.,  4., 11.,\n",
    "       13.,  7.,  6.,  4.,  8.,  6.,  9.,  6.,  9.,  4.,  5., 14.,  8.,\n",
    "        3., 13., 12.,  2.,  5., 14.,  5.,  8.,  7.,  6., 10.,  4.,  7.,\n",
    "        6.,  7., 11.,  4.,  7.,  5.,  6.,  6.,  3., 10.,  5.,  5.,  6.,\n",
    "        5., 10.,  9., 14.,  5., 13.,  6.,  6.,  7.,  6., 10.,  7.,  4.,\n",
    "        9.,  6.,  9.,  4., 12., 10.,  6.,  4.,  8.,  5.,  4.,  4.,  4.,\n",
    "        9., 12.,  4.,  4.,  6.,  4.,  9.,  4.,  6.,  9.,  8.,  7.,  5.,\n",
    "       10., 11.,  6.,  6.,  6.,  8.,  6.,  8., 11.,  8.,  9., 12.,  9.,\n",
    "        8.,  6., 12.,  9.,  9.,  6.,  5.,  6., 12., 10.,  4.,  5.,  5.,\n",
    "        9.,  6., 10., 12.,  5.,  6.,  4.,  4.,  5.,  9.,  6.,  6., 11.,\n",
    "        6.,  7.,  4.,  5.,  6., 10.,  8.,  5.,  4.,  5.,  8.,  8., 11.,\n",
    "        5., 11.,  9., 11., 11.,  6.,  8.,  6., 10.,  9.,  4., 12.,  7.,\n",
    "        6.,  6., 12., 10.,  4.,  6., 10.,  8.,  4.,  6.,  6.,  4.,  6.,\n",
    "       11., 12.,  5., 11., 12.,  5.,  8.,  8., 10.,  7.,  7.,  5., 13.,\n",
    "       12.,  5.,  4., 11.,  7., 13.,  5., 12.,  6.,  4.,  5.,  5.,  4.,\n",
    "       11.,  9.,  6., 10., 12.,  4.,  4., 11.,  5.,  4.,  9.,  4.,  9.,\n",
    "        6., 11.,  8.,  3.,  4.,  6.,  5.,  4.,  8.,  9., 10.,  6.,  7.,\n",
    "        5., 11.,  9.,  4.,  5.,  4.,  3.,  6.,  5.,  5.,  3.,  7.,  4.,\n",
    "        9.,  8.,  6.,  3.])\n",
    "\n",
    "fr_1 = torch.tensor([ 9.,  8.,  9., 10., 11.,  6.,  7.,  9., 10., 11.,  5.,  6.,  4.,\n",
    "        8.,  9.,  5.,  8.,  5.,  9.,  7.,  7.,  7.,  6., 10., 12., 10.,\n",
    "       11.,  6.,  5.,  8.,  9., 10., 10.,  9., 11.,  5.,  7.,  7.,  6.,\n",
    "        7.,  8.,  8.,  7.,  7.,  6.,  7.,  8.,  8.,  6.,  5., 13., 10.,\n",
    "        4.,  5.,  5.,  9., 10., 10., 12.,  5.,  6., 10., 11., 11., 10.,\n",
    "        7.,  6., 13.,  6., 11.,  8.,  7., 11., 14.,  6., 11., 10., 10.,\n",
    "        6.,  5., 12.,  7., 11., 12.,  9.,  9.,  9.,  9.,  8.,  7., 11.,\n",
    "       13.,  4.,  5., 10.,  9., 11.,  5.,  5.,  3.,  6.,  7.,  5., 10.,\n",
    "        8.,  8., 13., 11., 14., 12., 14.,  4., 11.,  4., 11., 12.,  9.,\n",
    "        6.,  5.,  8.,  7.,  6., 10.,  9.,  8., 11.,  5.,  6.,  7., 10.,\n",
    "        7., 11.,  5.,  9.,  8., 10., 12.,  9.,  8.,  7.,  9.,  6.,  5.,\n",
    "        5.,  9.,  6., 10.,  4., 12.,  9., 12.,  7.,  4.,  9.,  8., 12.,\n",
    "        6., 12., 10.,  2.,  6., 12.,  7.,  8.,  7.,  7.,  8.,  8., 12.,\n",
    "        7., 11., 10.,  7.,  9.,  8.,  8.,  5., 10., 10.,  8.,  6.,  4.,\n",
    "        3.,  8., 10.,  4.,  4., 10., 11.,  6., 11., 12.,  9.,  5.,  8.,\n",
    "        7.,  5.,  5.,  9.,  8.,  7.,  9.,  8., 10.,  6.,  6., 10.,  6.,\n",
    "        5.,  8., 10., 10.,  5.,  8., 10., 10.,  3., 10.,  4.,  5.,  6.,\n",
    "       13.,  7.,  4.,  9.,  7.,  7.,  8.,  3., 10., 12.,  6., 10., 11.,\n",
    "        8.,  9.,  5.,  6.,  4.,  7.,  6.,  6.,  6., 10.,  5., 10., 11.,\n",
    "        7.,  8.,  5.,  4.,  7.,  3.,  7.,  7.,  5.,  9., 11.,  6.,  5.,\n",
    "       11.,  8.,  6.,  5.,  5.,  9.,  9., 12.,  3.,  6.,  9.,  4.,  6.,\n",
    "        6., 11.,  4.,  9.,  9.,  8., 10., 12.,  7., 13.,  6.,  8.,  4.,\n",
    "       12.,  8.,  9.,  4.,  7.,  9.,  7., 10.,  9.,  6.,  9.,  6., 10.,\n",
    "        9.,  6.,  3.,  8.,  9.,  6., 13.,  9., 10.,  8.,  7.,  5.,  5.,\n",
    "        6.,  5.,  5.,  6., 12.,  7.,  6.,  9.,  6., 11.,  7.,  7., 11.,\n",
    "        4., 10.,  3.,  7., 10.,  7., 10., 10.,  7., 10.,  7.,  9., 10.,\n",
    "       13., 10.,  6.,  4., 10.,  3.,  9., 10.,  9.,  6.,  6.,  8., 10.,\n",
    "       10.,  5.,  8.,  9.,  9., 11.,  7.,  3.,  8., 11.,  8., 10., 10.,\n",
    "        8., 11.,  5.,  6.,  4., 10.,  4.,  7., 10.,  6.,  7.,  7., 10.,\n",
    "        5.,  6.,  8.,  8.,  4.,  8.,  4., 13.,  3., 12.,  4., 10., 10.,\n",
    "        6., 11.,  8., 10.,  5.,  8., 12.,  9.,  7., 10.,  9., 10.,  7.,\n",
    "        4.,  6.,  9.,  6.,  4.,  9., 11.,  8., 11.,  5.,  5.,  8., 12.,\n",
    "       11.,  4.,  5.,  8., 10.,  7., 12.,  8.,  6.,  7.,  6.,  4.,  6.,\n",
    "        6.,  7.,  9.,  9.,  8.,  6.,  8.,  6.,  4., 15., 11., 11., 11.,\n",
    "        5., 10.,  4.,  9.,  6., 14.,  7.,  5.,  6.,  8., 10.,  7., 10.,\n",
    "       10.,  9.,  9.,  5.,  6.,  9.,  7.,  8.,  5.,  5.,  8.,  6.,  8.,\n",
    "        7., 11.,  7.,  6.,  5., 13., 10.,  5.,  7.,  9.,  8.,  8.,  4.,\n",
    "        4., 12.,  6., 10., 10.,  8., 12., 10.,  6.,  7.,  5.,  6.,  9.,\n",
    "       10.,  4.,  5.,  9.,  4.,  4.,  8.,  7.,  8.,  6.,  5.,  8.,  7.,\n",
    "        7.,  9.,  7.,  7.,  4.,  5.,  9., 10.,  8.,  9., 10.,  7., 12.,\n",
    "        4., 14.,  4., 13.,  6.,  7., 11., 12.,  7.,  6.,  9.,  5., 11.,\n",
    "        9.,  4.,  9.,  7.,  9.,  4.,  8.,  9., 11.,  4.,  8.,  6.,  6.,\n",
    "       11.,  9., 10., 11.,  4., 10.,  9.,  7.,  7.,  6., 11.,  5.,  7.,\n",
    "        6., 10., 13., 12., 10., 11.,  5., 10., 10.,  6.,  8., 11.,  5.,\n",
    "       11., 10., 11.,  8.,  4.,  5.,  9., 10., 10.,  8.,  3., 10., 10.,\n",
    "       11., 10.,  6., 11.,  9.,  9., 10., 10.,  6.,  7.,  9.,  8.,  3.,\n",
    "        9.,  8.,  8.,  4., 10.,  9., 11., 10.,  8.,  8., 10.,  7., 10.,\n",
    "        5.,  9., 11.,  6.,  7., 10., 11.,  6., 10., 10.,  4., 11.,  6.,\n",
    "        6.,  5.,  9., 11., 10.,  9., 10.,  9.,  6.,  6.,  9.,  9., 12.,\n",
    "        7., 10.,  8.,  9.,  9.,  4., 10.,  9., 13.,  6.,  6.,  9., 11.,\n",
    "       11.,  5.,  8., 11., 15.,  9.,  4.,  9., 12.,  7.,  7.,  5., 13.,\n",
    "       11.,  9., 12.,  7.,  9., 10.,  7.,  9.,  5.,  9.,  5.,  4., 15.,\n",
    "        8.,  7., 11., 11., 11.,  5.,  5., 10.,  5.,  4.,  8.,  5.,  4.,\n",
    "        9.,  5.,  8.,  5.,  5.,  5.,  5., 11.,  4.,  8.,  4., 10.,  7.,\n",
    "        6., 10.,  5.,  8., 11.,  7.,  7.,  6.,  4., 11.,  9., 12.,  6.,\n",
    "       11.,  8., 11.,  8.,  5.,  6.,  8.,  4., 10.,  8.,  7.,  5.,  9.,\n",
    "        5.,  8.,  8.,  4., 11.,  9.,  8.,  6., 10.,  7.,  6.,  7.,  4.,\n",
    "        6.,  8., 11.,  9., 11.,  9.,  8.,  6.,  9., 12.,  6.,  5., 12.,\n",
    "        5.,  7., 13.,  8.,  7.,  7.,  9., 11.,  6.,  8., 10.,  9., 12.,\n",
    "        7.,  7., 10., 10., 10.,  6.,  6., 12.,  8.,  7.,  7.,  8.,  8.,\n",
    "        6., 10.,  8., 10.,  8.,  6.,  7.,  8.,  8.,  6.,  8.,  8., 11.,\n",
    "        7.,  7., 12.,  5., 10., 12.,  5.,  4.,  7., 15.,  7., 12., 11.,\n",
    "        9.,  5.,  5.,  9.,  5.,  4.,  6., 11.,  9.,  5.,  4.,  4.,  6.,\n",
    "       11.,  7.,  6.,  6.,  8.,  6.,  9.,  9.,  9.,  5.,  9.,  6., 12.,\n",
    "        9.,  6.,  6.,  8., 12.,  8.,  9.,  4.,  7.,  8., 10.,  5., 12.,\n",
    "        9., 11., 10.,  8.,  6., 11., 10.,  7.,  6., 13.,  6., 12., 11.,\n",
    "        8., 14.,  8., 11.,  6.,  7.,  5.,  7.,  8.,  7.,  7.,  8.,  7.,\n",
    "        5.,  4., 10., 12., 12.,  9., 12.,  9.,  8., 11., 11.,  8.,  5.,\n",
    "        8.,  5.,  9., 10.,  9., 10.,  4., 10., 10.,  6., 11.,  7.,  5.,\n",
    "        5.,  8., 10.,  7.,  6.,  9.,  7., 11.,  5.,  6., 14.,  4.,  9.,\n",
    "        6., 12.,  8., 11.,  6.,  9.,  4.,  8.,  7.,  5.,  7.,  6.,  8.,\n",
    "        8., 14.,  8., 13.,  8., 13.,  8.,  5., 10.,  4., 10.,  8.,  6.,\n",
    "       10.,  2.,  8.,  6., 13.,  9.,  6.,  9., 11.,  7., 10.,  6.,  8.,\n",
    "       10.,  4.,  5.,  4., 11., 13.,  4.,  5.,  9., 10.,  4., 11.,  4.,\n",
    "        9.,  4., 11.,  6.,  8.,  6.,  8., 13., 11.,  9.,  9.,  8., 11.,\n",
    "       11., 10.,  7.,  8.,  6., 10., 12.,  7.,  4.,  9., 10., 12.,  7.,\n",
    "        7.,  6.,  7., 13.,  5.,  8.,  7.,  5.,  6., 12., 11.,  6.,  8.,\n",
    "        9.,  8., 10.,  9., 11.,  9.,  4.,  8., 10.,  8.,  7.,  4.,  5.,\n",
    "       10.,  9.,  7., 10., 11., 10.,  6., 11.,  7.,  8.,  8.,  8.,  6.,\n",
    "        9.,  9.,  6.,  8.,  4.,  5., 11.,  7., 11.,  9.,  8., 11.,  9.,\n",
    "        5., 11., 14.,  5.,  5.,  9., 11., 13.,  6.,  5., 10.,  5.,  7.,\n",
    "        6., 12.,  6.,  9., 10.,  8.,  8.,  6.,  5.,  8., 12.,  6.,  6.,\n",
    "       11.,  9.,  9., 11.,  7.,  7.,  8.,  5.,  8.,  4.,  9.,  7., 11.,\n",
    "        9., 11.,  9., 11., 12.,  4.,  8., 10.,  6., 12.,  8., 11.,  8.,\n",
    "       10., 12., 12.,  7.,  8.,  8.,  7.,  9.,  7.,  9., 11.,  7.,  6.,\n",
    "        6.,  6.,  6.,  6.,  8.,  6.,  6.,  9.,  2.,  8.,  6.,  5.,  8.,\n",
    "        9.,  8., 10.,  8.,  8., 10., 11., 11.,  9.,  9.,  7.,  8.,  5.,\n",
    "        9.,  6.,  5., 13.,  9.,  8.,  6.,  7.,  9.,  6.,  8., 13.,  8.,\n",
    "        5.,  5., 10.,  5., 11.,  7., 10.,  8.,  6.,  4., 11.,  5.,  5.,\n",
    "       10., 12.,  7.,  6.,  5.,  4.,  8., 10.,  8.,  9.,  8.,  8.,  8.,\n",
    "       14.,  8., 11.,  8., 13.,  7.,  8.,  6., 12.,  7., 12., 11.,  7.,\n",
    "        7.,  6.,  9.,  9., 11.,  9.,  5.,  4.,  5., 10.,  8., 10., 11.,\n",
    "       10.,  4., 12.,  4.,  7., 10.,  5.,  6.,  9., 11.,  7., 10.,  8.,\n",
    "        7.,  9.,  8.,  6., 11.,  7., 11.,  9.,  7.,  6., 10.,  9.,  8.,\n",
    "        7.,  6.,  8., 11., 11.,  8., 10.,  9.,  6., 12.,  6.,  7., 10.,\n",
    "       10., 11., 11., 10.,  5.,  5.,  4., 11., 10.,  8., 12., 10., 10.,\n",
    "        8.,  3.,  6.,  9., 10., 10.,  5.,  7.,  5.,  9.,  7.,  6.,  5.,\n",
    "        9.,  3.,  3.,  9.,  8.,  7.,  6.,  5.,  5.,  5.,  8.,  5.,  7.,\n",
    "       11.,  8., 10., 10.,  7.,  7.,  4.,  5., 10.,  6.,  7.,  7.,  6.,\n",
    "        4.,  4.,  9.,  6., 10.,  4.,  7.,  6.,  4., 11.,  6.,  7.,  7.,\n",
    "       14., 11.,  8.,  7.,  8.,  5.,  7., 10.,  9.,  7.,  9.,  7., 12.,\n",
    "       10., 12.,  7.,  6.,  9.,  5., 10., 11., 12.,  9.,  9.,  6., 10.,\n",
    "        6.,  5., 14., 13., 10., 11.,  6.,  6.,  4.,  8.,  4., 11.,  6.,\n",
    "       11., 14., 10., 11.,  8.,  9.,  8.,  6.,  7., 11.,  7., 10.,  6.,\n",
    "        8., 11.,  9., 11., 10.,  7., 10.,  6.,  8., 12.,  6.,  4.,  5.,\n",
    "        4.,  8.,  4., 10., 10.,  4., 11., 11.,  9., 11.,  5.,  9.,  3.,\n",
    "       10., 10.,  4.,  9.,  7., 13.,  6., 11., 10.,  7.,  6.,  9., 10.,\n",
    "        2.,  9.,  6., 12.,  8.,  4., 11.,  7.,  4., 12., 10.,  9.,  9.,\n",
    "        9.,  4.,  5., 10.,  6.,  9.,  4.,  8.,  4., 12.,  7.,  6., 10.,\n",
    "       11.,  9., 10.,  9.,  5.,  5., 11.,  8.,  6.,  5.,  6.,  7.,  6.,\n",
    "        8.,  9.,  9.,  4.,  4.,  5.,  5.,  9., 10.,  6., 10., 10.,  4.,\n",
    "        8.,  8., 12.,  8.,  4.,  4.,  9., 10.,  9., 10., 11., 10.,  9.,\n",
    "       11., 13., 12.,  7.,  6., 11.,  8.,  4.,  5.,  6.,  9., 10., 12.,\n",
    "        8.,  8.,  8., 10.,  4.,  8., 10.,  8.,  6.,  7., 10., 10.,  9.,\n",
    "        9.,  7.,  7., 10.,  7., 12.,  6.,  6.,  4., 12.,  6.,  7., 13.,\n",
    "        7.,  8.,  5.,  9., 10.,  6.,  7.,  5.,  4.,  6.,  6.,  9.,  6.,\n",
    "       10., 11.,  5.,  7.,  6.,  8.,  4.,  8.,  4.,  8., 13.,  7., 11.,\n",
    "        9., 12.,  5., 11.,  8.,  9.,  6.,  6.,  6.,  7.,  5., 10.,  7.,\n",
    "       10.,  9., 10.,  7.,  4.,  6.,  6., 12., 10.,  6.,  4., 12.,  5.,\n",
    "        7.,  7.,  5.,  7.,  5.,  7., 10.,  7.,  9., 10.,  8., 12., 10.,\n",
    "       10., 13.,  7.,  8., 10.,  8.,  6.,  5., 11.,  4.,  7.,  9.,  6.,\n",
    "        6.,  6.,  9.,  8., 12.,  9.,  8., 10.,  7.,  6.,  5.,  8.,  6.,\n",
    "       10., 10., 11., 14.,  5.,  8.,  5.,  5.,  9.,  8., 11.,  9.,  9.,\n",
    "       10., 12.,  9., 10.,  6., 10.,  4.,  6.,  8.,  6., 10., 10., 11.,\n",
    "        6.,  9.,  6.,  9.,  6.,  9.,  6.,  8.,  8., 11.,  8.,  7.,  9.,\n",
    "       10.,  5., 11.,  4.,  7.,  7.,  7., 12.,  5.,  4., 12.,  9., 11.,\n",
    "        6., 10.,  7.,  5.,  9., 12.,  8.,  5., 12.,  9.,  8.,  5., 10.,\n",
    "        8.,  9.,  7.,  7.,  7.,  8., 11.,  9.,  5., 10., 11.,  6., 10.,\n",
    "        6., 10.,  8.,  4.,  6.,  7., 11., 10., 11., 10., 11.,  7., 12.,\n",
    "        4.,  8.,  5., 11.,  8.,  8.,  8., 11.,  7.,  7.,  7.,  8.,  9.,\n",
    "        7.,  9., 10., 11.,  9., 10., 12.,  5.,  8., 11.,  4.,  6.,  3.,\n",
    "       10., 11., 12.,  5., 10.,  8.,  9.,  4.,  8.,  8.,  8.,  6.,  7.,\n",
    "        7.,  2., 14.,  7.,  6.,  8.,  8.,  8.,  7.,  9., 11.,  4.,  6.,\n",
    "        9.,  9.,  9., 12.,  5.,  7.,  4.,  8.,  6.,  6.,  5.,  6.,  9.,\n",
    "        8., 10.,  8., 11.,  8.,  9.,  4.,  9., 10.,  7.,  7.,  4., 10.,\n",
    "       12.,  5., 11., 11.,  6.,  6.,  9.,  4.,  8.,  9.,  5., 12., 11.,\n",
    "        9.,  8.,  7.,  5.,  7., 12.,  7., 12.,  5., 11., 12., 10.,  6.,\n",
    "        4.,  4.,  9.,  9., 10.,  8.,  8.,  8.,  9.,  8., 12.,  8.,  6.,\n",
    "        7., 10., 11.,  9., 13.,  6.,  5., 12., 11.,  9.,  9.,  8.,  7.,\n",
    "        7.,  8.,  8.,  4., 10., 12., 10.,  6., 10.,  9.,  7.,  9., 11.,\n",
    "       10.,  8.,  8.,  7., 10., 10.,  4., 11.,  8.,  9., 11.,  9., 11.,\n",
    "       11.,  5.,  8.,  7., 10., 12., 11., 11.,  3.,  8.,  7., 13.,  2.,\n",
    "        4., 10.,  4.,  9.,  7.,  8.,  7.,  7.,  4.,  9.,  8., 10.,  7.,\n",
    "        6.,  9.,  8.,  7., 14.,  8.,  6., 10., 11.,  7.,  7.,  4.,  8.,\n",
    "        4.,  7., 11.,  9., 10.,  7., 10.,  4.,  8.,  3.,  5.,  6.,  4.,\n",
    "       10.,  6.,  9.,  6., 11.,  8.,  9.,  3.,  7.,  8.,  9., 10.,  9.,\n",
    "        8.,  9.,  8.,  8.,  9.,  7.,  9.,  9., 11.,  4.,  9.,  9.,  7.,\n",
    "        9., 13.,  8.,  9., 10.,  4.,  6.,  9.,  9.,  9.,  5.,  6.,  6.,\n",
    "       10., 10.,  4.,  4.,  8.,  9.,  3., 11.,  6.,  7.,  9.,  9.,  5.,\n",
    "        3., 11.,  9.,  6.,  5.,  6.,  9.,  9., 11.,  6., 11.,  8.,  9.,\n",
    "        9.,  7.,  8.,  7.,  9.,  6., 13., 11.,  7., 13.,  7.,  8.,  9.,\n",
    "       13.,  9.,  8.,  8., 10., 11., 10.,  6.,  9.,  7., 10.,  9.,  7.,\n",
    "       10.,  5.,  6.,  8.,  9.,  6.,  8.,  8.,  7.,  9.,  8.,  8.,  6.,\n",
    "       10.,  8.,  9.,  7.,  8.,  9.,  8.,  2.,  5.,  6.,  5., 12., 12.,\n",
    "        7.,  4.,  7.,  8., 10.,  4.,  8.,  9.,  9., 10.,  9.,  4.,  8.,\n",
    "       11.,  4.,  7.,  8.])\n",
    "\n",
    "targets_acc_test = torch.tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
    "        1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
    "        0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
    "        0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n",
    "        1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
    "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
    "        0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0,\n",
    "        1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1,\n",
    "        1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
    "        0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
    "        1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
    "        0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
    "        1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
    "        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,\n",
    "        1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
    "        1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0,\n",
    "        1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
    "        0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
    "        1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
    "        1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0,\n",
    "        0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "        0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
    "        0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
    "        0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
    "        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "        1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
    "        0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
    "        1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
    "        0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
    "        1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
    "        0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
    "        0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
    "        0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
    "        0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
    "        0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
    "        1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
    "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
    "        1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
    "        0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0,\n",
    "        1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
    "        1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0,\n",
    "        1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
    "        0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0,\n",
    "        0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
    "        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
    "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
    "        0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
    "        0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
    "        1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "        0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0,\n",
    "        0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
    "        0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
    "        0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1,\n",
    "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
    "        1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
    "        0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
    "        1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
    "        0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
    "        1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n",
    "        0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
    "        0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
    "        0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n",
    "        0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1,\n",
    "        1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
    "        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
    "        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
    "        0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
    "        1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
    "        0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0,\n",
    "        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
    "        1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
    "        0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
    "        1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
    "        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
    "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
    "        1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
    "        0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
    "        0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
    "        1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
    "        1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
    "        1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
    "        0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
    "        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
    "        0, 1, 1])\n",
    "\n",
    "# generating a polpulation encoded data with size [batch_size, num_classes]\n",
    "pop_code = torch.stack([fr_0, fr_1], dim=1)\n",
    "print(\"pop_code shape:\", pop_code.shape)\n",
    "_, idx = pop_code.max(1)\n",
    "accuracy = np.mean((targets_acc_test == idx).detach().cpu().numpy())\n",
    "print(\"Hardware Accuracy = \", accuracy*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
