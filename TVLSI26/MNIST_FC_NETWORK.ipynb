{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c682041",
   "metadata": {},
   "source": [
    "For this network configure the constants as follows:\n",
    "- population_code = False\n",
    "- min_weight = - 0.386\n",
    "- max_weight = 0.386\n",
    "- image_size = 28\n",
    "- num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from snntorch import functional as SF\n",
    "from snntorch import spikegen\n",
    "from torchvision import datasets\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2daba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TVLSI26.datsets.mnist_dataset import MNISTdata\n",
    "from TVLSI26.configs.config import modelConstants\n",
    "from TVLSI26.neuron_models.digLIF import digLIF, Square\n",
    "from TVLSI26.ctt_weights.weight_variations import maskW, WeightDropoutLinear, apply_quant_noise, add_retention_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433086c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = int(time.time() * 1000) ^ random.getrandbits(32)\n",
    "counter_global = 0\n",
    "data_path='/home/zmoham13/pydata'\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568b963",
   "metadata": {},
   "source": [
    "# Network and Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "mnist_dataset = MNISTdata(data_path=modelConstants.data_path, FULL_MNIST=True, image_size=modelConstants.image_size)\n",
    "train_loader = DataLoader(mnist_dataset.get_train_data(), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_dataset.get_test_data(), batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "\n",
    "for dataset_name, dataset in zip([\"mnist_train\", \"mnist_test\"], (mnist_dataset.get_train_data(), mnist_dataset.get_test_data())):\n",
    "    print(f\"Number of images in {dataset_name}: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fe6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNfullyConnected(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_inputs=28*28,\n",
    "                 h1=256,\n",
    "                 h2=128,\n",
    "                 num_outputs=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_inputs  = num_inputs\n",
    "        self.h1          = h1\n",
    "        self.h2          = h2\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # 784 -> 512 -> 256 -> 10\n",
    "        self.fc1 = WeightDropoutLinear(self.num_inputs,  self.h1,          bias=False)\n",
    "        self.fc2 = WeightDropoutLinear(self.h1,          self.h2,          bias=False)\n",
    "        self.fc3 = WeightDropoutLinear(self.h2,          self.num_outputs, bias=False)\n",
    "\n",
    "        self.lif1 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "        self.lif2 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "        self.lif3 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # keep centered in your tiny range\n",
    "        mean = (modelConstants.max_weight + modelConstants.min_weight) / 2.0\n",
    "        std  = (modelConstants.max_weight - modelConstants.min_weight) / 8.0\n",
    "\n",
    "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
    "            nn.init.normal_(layer.linear.weight, mean=mean, std=std)\n",
    "            with torch.no_grad():\n",
    "                layer.linear.weight.clamp_(modelConstants.min_weight, modelConstants.max_weight)\n",
    "\n",
    "    def _noisy_weight(self, base_weight):\n",
    "        w = base_weight\n",
    "        if self.training and modelConstants.training_noise_ctt:\n",
    "            if modelConstants.retention_noise:\n",
    "                w = add_retention_noise(w, modelConstants.std_ret_high, modelConstants.std_ret_low)\n",
    "            if modelConstants.quantization_noise:\n",
    "                w = apply_quant_noise(w)\n",
    "        return w\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [T, B, 1, 28, 28] or [T, B, 28, 28] or [T, B, 784]\n",
    "\n",
    "        returns:\n",
    "            spk_rec_3: [T, B, 10]\n",
    "            mem_rec_3: [T, B, 10]\n",
    "        \"\"\"\n",
    "\n",
    "        T = x.shape[0]\n",
    "\n",
    "        # Flatten spatial dims\n",
    "        if x.dim() == 5:          # [T, B, C, H, W]\n",
    "            x_flat = x.view(T, x.shape[1], -1)\n",
    "        elif x.dim() == 4:        # [T, B, H, W]\n",
    "            x_flat = x.view(T, x.shape[1], -1)\n",
    "        elif x.dim() == 3:        # [T, B, F]\n",
    "            x_flat = x\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape {x.shape}\")\n",
    "\n",
    "        # init membrane\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # sample weight noise for this forward\n",
    "        self.nW1 = torch.normal(torch.zeros_like(self.fc1.linear.weight), modelConstants.std_ctt)\n",
    "        self.nW2 = torch.normal(torch.zeros_like(self.fc2.linear.weight), modelConstants.std_ctt)\n",
    "        self.nW3 = torch.normal(torch.zeros_like(self.fc3.linear.weight), modelConstants.std_ctt)\n",
    "\n",
    "        # effective weights\n",
    "        noisy_w1 = self._noisy_weight(self.fc1.linear.weight)\n",
    "        noisy_w2 = self._noisy_weight(self.fc2.linear.weight)\n",
    "        noisy_w3 = self._noisy_weight(self.fc3.linear.weight)\n",
    "\n",
    "        spk_rec_3, mem_rec_3 = [], []\n",
    "\n",
    "        for step in range(T):\n",
    "            spk0 = x_flat[step]                      # [B, 784]\n",
    "\n",
    "            # layer 1\n",
    "            cur1 = Square.apply(spk0, noisy_w1)\n",
    "            with torch.no_grad():\n",
    "                noise1 = nn.functional.linear(spk0, self.nW1)\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur1 = cur1 + noise1\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            # layer 2\n",
    "            cur2 = Square.apply(spk1, noisy_w2)\n",
    "            with torch.no_grad():\n",
    "                noise2 = nn.functional.linear(spk1, self.nW2)\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur2 = cur2 + noise2\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            # layer 3 (output)\n",
    "            cur3 = Square.apply(spk2, noisy_w3)\n",
    "            with torch.no_grad():\n",
    "                noise3 = nn.functional.linear(spk2, self.nW3)\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur3 = cur3 + noise3\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk_rec_3.append(spk3)\n",
    "            mem_rec_3.append(mem3)\n",
    "\n",
    "        spk_rec_3 = torch.stack(spk_rec_3, dim=0)   # [T, B, 10]\n",
    "        mem_rec_3 = torch.stack(mem_rec_3, dim=0)   # [T, B, 10]\n",
    "\n",
    "        return spk_rec_3, mem_rec_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "net = NNfullyConnected(28*28, 256, 256, 10).to(device)\n",
    "with torch.no_grad():\n",
    "    for layer in [net.fc1, net.fc2, net.fc3]:\n",
    "        layer.linear.weight.copy_(\n",
    "            torch.rand_like(layer.linear.weight) * (modelConstants.max_weight - modelConstants.min_weight) + modelConstants.min_weight\n",
    "        )\n",
    "\n",
    "for lif in [net.lif1, net.lif2, net.lif3]:\n",
    "    lif.threshold.data = (lif.threshold * modelConstants.Threshold_voltage).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "ce = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395b8871",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = sum(p.numel() for p in net.parameters())\n",
    "print(f\"Total parameters: {n_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "seed = int(time.time() * 1000) ^ random.getrandbits(32)\n",
    "torch.manual_seed(seed)\n",
    "counter = 0\n",
    "\n",
    "def train_and_evaluate(loss_fn = ce, num_epochs = 5, optimizer = optimizer, scheduler=scheduler, train_loader=train_loader, test_loader=test_loader):\n",
    "    epoch_times = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_batch = iter(train_loader)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            data = spikegen.latency(data, num_steps=modelConstants.num_steps, threshold=0.01, clip=True, first_spike_time=0, linear=True, normalize=True).cumsum(0)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            net.train()\n",
    "            spk_rec, _ = net(data)\n",
    "\n",
    "            loss_val = torch.zeros((1,), dtype=dtype, device=device)\n",
    "\n",
    "            logits = spk_rec.sum(dim=0)\n",
    "            loss_val = loss_fn(logits, targets)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for layer in [net.fc1, net.fc2, net.fc3]:\n",
    "                    layer.linear.weight.clamp_(modelConstants.min_weight, modelConstants.max_weight)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_times.append(end_time - start_time)\n",
    "        print(\"epoch \", epoch, \" loss: \", loss_val)\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            for data, targets in test_loader:\n",
    "                data = data.to(device)\n",
    "                data = spikegen.latency(data, num_steps=modelConstants.num_steps, threshold=0.01, clip=True, first_spike_time=0, linear=True, normalize=True).cumsum(0)\n",
    "                targets = targets.to(device)\n",
    "                spk_rec, _ = net(data)\n",
    "\n",
    "                logits = spk_rec.sum(dim=0)\n",
    "                predicted = logits.argmax(dim=-1)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "            print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "            print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\\n\")\n",
    "            for param_group in optimizer.param_groups:\n",
    "                current_lr = param_group['lr']\n",
    "                print(f\"Current learning rate: {current_lr}\")\n",
    "            '''with open(\"model_accuracy.txt\", \"a\") as f:\n",
    "                f.write(f\"{epoch}: {100 * correct / total:.2f}%\\n\")'''\n",
    "    print(\"average epoch time: \", np.mean(epoch_times))\n",
    "    return 100 *(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ad145",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "best_params = None\n",
    "worst_accuracy = 100 \n",
    "\n",
    "thr_values = [0.4]\n",
    "\n",
    "hyperparams = [{\"thr\": thr} for thr in thr_values]\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=2,          # first cycle length\n",
    "    T_mult=1,       # cycle length stays constant\n",
    "    eta_min=1e-5    # small floor, not zero\n",
    ")\n",
    "\n",
    "for params in hyperparams:\n",
    "    V_th = params[\"thr\"]\n",
    "    net.lif1.threshold.data = torch.tensor(V_th)\n",
    "    accuracy = train_and_evaluate(loss_fn=ce, num_epochs = 30, optimizer = optimizer, scheduler=scheduler)\n",
    "    print(f\"Accuracy with params {params}: {accuracy}\")\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "        best_weights = net.fc1.linear.weight.clone()\n",
    "    if accuracy < worst_accuracy:\n",
    "        worst_accuracy = accuracy\n",
    "        worst_params = params\n",
    "        \n",
    "    print(f\"Best parameters: {best_params} with accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8be8bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOISY  test acc: 97.51% | loss: 0.0921\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def clone_model_fp(model_fp: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Rebuilds NNfullyConnected using the SAME hyperparams stored in the instance,\n",
    "    then loads state_dict. This avoids deepcopy() issues.\n",
    "    \"\"\"\n",
    "    device = next(model_fp.parameters()).device\n",
    "\n",
    "    # Your NN stores these attributes in __init__ :contentReference[oaicite:2]{index=2}\n",
    "    model_copy = NNfullyConnected(\n",
    "        num_inputs=model_fp.num_inputs,\n",
    "        h1=model_fp.h1,\n",
    "        h2=model_fp.h2,\n",
    "        num_outputs=model_fp.num_outputs,\n",
    "    ).to(device)\n",
    "\n",
    "    model_copy.load_state_dict(model_fp.state_dict(), strict=True)\n",
    "    model_copy.eval()\n",
    "    return model_copy\n",
    "\n",
    "# Quantization copy (no deepcopy)\n",
    "def quantize_tensor_uniform(w: torch.Tensor, lo: float, hi: float, levels: int) -> torch.Tensor:\n",
    "    w = w.clamp(lo, hi)\n",
    "    step = (hi - lo) / (levels - 1)\n",
    "    return torch.round((w - lo) / step) * step + lo\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_quantization_copy(model_fp: nn.Module, lo: float, hi: float, levels: int, include_bias: bool = True):\n",
    "    \"\"\"\n",
    "    Returns a quantized COPY of model_fp (model_fp is untouched).\n",
    "    Implemented WITHOUT deepcopy() to avoid:\n",
    "    RuntimeError: Only Tensors created explicitly by the user support deepcopy...\n",
    "    \"\"\"\n",
    "    model_q = clone_model_fp(model_fp)\n",
    "\n",
    "    # Your weights live in WeightDropoutLinear.linear.weight :contentReference[oaicite:3]{index=3}\n",
    "    for layer in [model_q.fc1, model_q.fc2, model_q.fc3]:\n",
    "        layer.linear.weight.copy_(quantize_tensor_uniform(layer.linear.weight, lo=lo, hi=hi, levels=levels))\n",
    "        if include_bias and layer.linear.bias is not None:\n",
    "            layer.linear.bias.copy_(quantize_tensor_uniform(layer.linear.bias, lo=lo, hi=hi, levels=levels))\n",
    "\n",
    "    return model_q\n",
    "\n",
    "# Noise-only: apply SubthresholdPVTNoise to weights\n",
    "@torch.no_grad()\n",
    "def apply_subthreshold_noise_copy(model_fp: nn.Module, noise_module: nn.Module):\n",
    "    \"\"\"\n",
    "    Copies model_fp, applies your SubthresholdPVTNoise() to the synaptic weights only,\n",
    "    clamps to [min_weight, max_weight], returns noisy model.\n",
    "    \"\"\"\n",
    "    model_n = clone_model_fp(model_fp)\n",
    "\n",
    "    # Apply noise to *weights only* (fc1/fc2/fc3)\n",
    "    for layer in [model_n.fc1, model_n.fc2, model_n.fc3]:\n",
    "        w = layer.linear.weight\n",
    "        w_noisy = noise_module(w)          # <-- THIS is your noise function call\n",
    "        w.copy_(w_noisy)\n",
    "        w.clamp_(modelConstants.min_weight, modelConstants.max_weight)   # keep same hardware range\n",
    "\n",
    "    model_n.eval()\n",
    "    return model_n\n",
    "\n",
    "\n",
    "# One-call: noisy weights inference (original net untouched)\n",
    "@torch.no_grad()\n",
    "def evaluate_with_noisy_weights(net_fp: nn.Module, test_loader, noise_module: nn.Module):\n",
    "    \"\"\"\n",
    "    Makes a noisy-weight copy and runs your existing infer_accuracy_snn().\n",
    "    \"\"\"\n",
    "    net_noisy = apply_subthreshold_noise_copy(net_fp, noise_module)\n",
    "    noisy_loss, noisy_acc = infer_accuracy_snn(net_noisy, test_loader)\n",
    "    return noisy_loss, noisy_acc\n",
    "\n",
    "\n",
    "noise = SubthresholdPVTNoise().to(device)   # if your class is nn.Module\n",
    "n_loss, n_acc = evaluate_with_noisy_weights(net, test_loader, noise)\n",
    "print(f\"NOISY  test acc: {n_acc*100:.2f}% | loss: {n_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438cdd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32 test acc:  97.49%\n",
      "QUANT test acc: 97.64% (levels=24, range=[-0.386,0.386])\n",
      "Sanity: max|fc1_fp - fc1_quant| = 0.016782\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch.nn as nn\n",
    "\n",
    "@torch.no_grad()\n",
    "def quantize_model_copy(model_fp: nn.Module,\n",
    "                        lo: float,\n",
    "                        hi: float,\n",
    "                        levels: int,\n",
    "                        include_bias: bool = True) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Returns a quantized deep-copy of model_fp. model_fp is NOT modified.\n",
    "    Quantizes nn.Linear weights (and optional bias). Skips BatchNorm params.\n",
    "    \"\"\"\n",
    "    model_q = clone_model_fp(model_fp).to(device)\n",
    "    model_q.eval()\n",
    "\n",
    "    step = (hi - lo) / (levels - 1)\n",
    "\n",
    "    for mod in model_q.modules():\n",
    "        # only quantize synapses; your synapses are inside WeightDropoutLinear.linear (nn.Linear)\n",
    "        if isinstance(mod, nn.Linear):\n",
    "            # weight\n",
    "            w = mod.weight\n",
    "            w.clamp_(lo, hi)\n",
    "            w.copy_(maskW(w))\n",
    "\n",
    "            # optional bias\n",
    "            if include_bias and (mod.bias is not None):\n",
    "                b = mod.bias\n",
    "                b.clamp_(lo, hi)\n",
    "                b.copy_(torch.round((b - lo) / step) * step + lo)\n",
    "\n",
    "    return model_q\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_accuracy_snn(model: nn.Module, loader, population_code: bool = False):\n",
    "    model.eval()\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    for data, targets in loader:\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # match your preprocessing\n",
    "        data = spikegen.latency(\n",
    "            data, num_steps=modelConstants.num_steps, threshold=0.01, clip=True,\n",
    "            first_spike_time=0, linear=True, normalize=True\n",
    "        ).cumsum(0)\n",
    "\n",
    "        spk_rec, _ = model(data)\n",
    "\n",
    "        logits = spk_rec.sum(dim=0)          # [B, 10]\n",
    "        pred = logits.argmax(dim=-1)         # [B]\n",
    "        correct += (pred == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    return 100.0 * float(correct) / float(total)\n",
    "\n",
    "\n",
    "# --- Build quantized copy + run inference ---\n",
    "levels = modelConstants.num_w_levels\n",
    "net_q = quantize_model_copy(net, lo=modelConstants.min_weight, hi=modelConstants.max_weight, levels=levels, include_bias=True)\n",
    "\n",
    "acc_fp = infer_accuracy_snn(net, test_loader)\n",
    "acc_q  = infer_accuracy_snn(net_q, test_loader)\n",
    "\n",
    "print(f\"FP32 test acc:  {acc_fp:.2f}%\")\n",
    "print(f\"QUANT test acc: {acc_q:.2f}% (levels={levels}, range=[{modelConstants.min_weight},{modelConstants.max_weight}])\")\n",
    "\n",
    "# Optional sanity check: prove the original net weights didn't change\n",
    "# (prints max absolute diff for fc1 weights; should be 0)\n",
    "with torch.no_grad():\n",
    "    diff = (net.fc1.linear.weight - net_q.fc1.linear.weight).abs().max().item()\n",
    "print(f\"Sanity: max|fc1_fp - fc1_quant| = {diff:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfa175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def optuna_objective(trial):\n",
    "    global net, optimizer, scheduler, num_steps\n",
    "\n",
    "    h1 = trial.suggest_int(\"h1\", 256, 512, step=256)\n",
    "    h2 = trial.suggest_int(\"h2\", 128, 256, step=128)\n",
    "\n",
    "    thr_scale = trial.suggest_float(\"thr_scale\", 0.2, 0.6, step=0.1)\n",
    "    num_steps = trial.suggest_int(\"num_steps\", 10, 40, step=10)\n",
    "\n",
    "    #lr = trial.suggest_int(\"lr\", 10e-5, 10e-2)\n",
    "    # number of epochs per trial (keep small for speed)\n",
    "    num_epochs = 4\n",
    "    #thr_scale = 0.3\n",
    "\n",
    "    net = NNfullyConnected(\n",
    "        num_inputs=28*28,\n",
    "        h1=h1,\n",
    "        h2=h2,\n",
    "        num_outputs=modelConstants.num_classes,\n",
    "    ).to(device)\n",
    "\n",
    "    for lif in [net.lif1, net.lif2, net.lif3]:\n",
    "        lif.threshold = (lif.threshold * thr_scale * modelConstants.Threshold_voltage).to(device)\n",
    "\n",
    "    # optimizer + scheduler\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=2)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=2,          # first cycle length\n",
    "        T_mult=1,       # cycle length stays constant\n",
    "        eta_min=1e-5    # small floor, not zero\n",
    "    )\n",
    "#     scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "#     optimizer,\n",
    "#     base_lr=1e-4,\n",
    "#     max_lr=1e-3,\n",
    "#     step_size_up=1000,   # batches\n",
    "#     mode='triangular2',\n",
    "#     cycle_momentum=False\n",
    "# )\n",
    "\n",
    "\n",
    "    # ensure all noise flags are OFF for this phase\n",
    "    # (you said noise is disabled; keep it that way for BO)\n",
    "    # training_noise_ctt = False\n",
    "    # general_noise = False\n",
    "    # retention_noise = False\n",
    "    # quantization_noise = False\n",
    "\n",
    "    # ---------- Train & evaluate ----------\n",
    "    # population_code=False so we use CE on spike counts\n",
    "    acc = train_and_evaluate(\n",
    "        loss_fn=ce,\n",
    "        num_epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e912198",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optuna_objective, n_trials=20)  # start with 20; increase if useful\n",
    "\n",
    "print(\"Best value (accuracy):\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in study.trials:\n",
    "    print(t.number, t.value, t.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc31e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"/home/zmoham13/model_weights\", exist_ok=True)\n",
    "torch.save(net.state_dict(), \"/home/zmoham13/model_weights/fc_net_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24dc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = study.best_params\n",
    "\n",
    "h1 = best[\"h1\"]\n",
    "h2 = best[\"h2\"]\n",
    "lr = best[\"lr\"]\n",
    "thr_scale = best[\"thr_scale\"]\n",
    "num_steps = best[\"num_steps\"]\n",
    "\n",
    "net = NNfullyConnected(\n",
    "    num_inputs=28*28,\n",
    "    h1=h1,\n",
    "    h2=h2,\n",
    "    num_outputs=modelConstants.num_classes,\n",
    ").to(device)\n",
    "\n",
    "for lif in [net.lif1, net.lif2, net.lif3]:\n",
    "    lif.threshold = (lif.threshold * thr_scale * modelConstants.Threshold_voltage).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "final_acc = train_and_evaluate(\n",
    "    loss_fn=ce,\n",
    "    num_epochs=15,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "\n",
    "print(\"Final accuracy with best params:\", final_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rezvan-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
