{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from snntorch import spikegen\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92807d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TVLSI26.datsets.mnist_dataset import MNISTdata\n",
    "from TVLSI26.configs.config import modelConstants\n",
    "from TVLSI26.neuron_models.digLIF import digLIF, Square\n",
    "from TVLSI26.ctt_weights.weight_variations import maskW, WeightDropoutLinear, apply_quant_noise, add_retention_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72087e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = int(time.time() * 1000) ^ random.getrandbits(32)\n",
    "counter_global = 0\n",
    "results_path = \"C:/Users/rezva/Documents/hardwareAwareLearning/results/\"\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43de6f",
   "metadata": {},
   "source": [
    "For this network configure the constants as follows:\n",
    "- population_code = False\n",
    "- min_weight = 0.02\n",
    "- max_weight = 0.8\n",
    "- image_size = 28\n",
    "- num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b7374",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "mnist_dataset = MNISTdata(data_path=modelConstants.data_path, FULL_MNIST=True, image_size=modelConstants.image_size)\n",
    "train_loader = DataLoader(mnist_dataset.get_train_data(), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_dataset.get_test_data(), batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "data = iter(train_loader)\n",
    "data_it, targets_it = next(data)\n",
    "\n",
    "for dataset_name, dataset in zip([\"mnist_train\", \"mnist_test\"], (mnist_dataset.get_train_data(), mnist_dataset.get_test_data())):\n",
    "    print(f\"Number of images in {dataset_name}: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fe6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG5_SNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Feature extraction ---\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, padding=1, bias=False)\n",
    "        self.lif1 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1, bias=False)\n",
    "        self.lif2 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1, bias=False)\n",
    "        self.lif3 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.fc = nn.Linear(256*7*7, num_classes, bias=False)\n",
    "        self.lif_out = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # match your existing init scheme\n",
    "        mean = (modelConstants.max_weight + modelConstants.min_weight) / 2.0\n",
    "        std = (modelConstants.max_weight - modelConstants.min_weight) / 8.0\n",
    "\n",
    "        for layer in [self.conv1, self.conv2, self.conv3, self.fc]:\n",
    "            nn.init.normal_(layer.weight, mean=mean, std=std)\n",
    "            with torch.no_grad():\n",
    "                layer.weight.clamp_(modelConstants.min_weight, modelConstants.max_weight)\n",
    "\n",
    "    def _noisy_weight(self, base_weight):\n",
    "        w = base_weight\n",
    "        if self.training and modelConstants.training_noise_ctt:\n",
    "            if modelConstants.retention_noise:\n",
    "                w = add_retention_noise(w, modelConstants.std_ret_high, modelConstants.std_ret_low)\n",
    "            if modelConstants.quantization_noise:\n",
    "                w = apply_quant_noise(w)\n",
    "        return w\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [T, B, 1, 28, 28]\n",
    "        T, B, C, H, W = x.shape\n",
    "        x = x.reshape(T * B, 1, H, W)\n",
    "\n",
    "        # Sample weight noise\n",
    "        # self.nW1 = torch.normal(torch.zeros_like(self.conv1.weight), std_ctt)\n",
    "        # self.nW2 = torch.normal(torch.zeros_like(self.conv2.weight), std_ctt)\n",
    "        # self.nW3 = torch.normal(torch.zeros_like(self.conv3.weight), std_ctt)\n",
    "        # self.nWF = torch.normal(torch.zeros_like(self.fc.weight),   std_ctt)\n",
    "\n",
    "        # w1 = self._noisy_weight(self.conv1.weight)\n",
    "        # w2 = self._noisy_weight(self.conv2.weight)\n",
    "        # w3 = self._noisy_weight(self.conv3.weight)\n",
    "        # wF = self._noisy_weight(self.fc.weight)\n",
    "\n",
    "        z1 = self.conv1(x)                     # (T·B,64,28,28)\n",
    "        z1 = z1.reshape(T, B, 64, 28, 28)\n",
    "        spk1, mem1 = [], self.lif1.init_leaky()\n",
    "\n",
    "        for t in range(T):\n",
    "            s, mem1 = self.lif1(z1[t], mem1)\n",
    "            spk1.append(s)\n",
    "\n",
    "        spk1 = torch.stack(spk1)               # (T,B,64,28,28)\n",
    "        p1 = self.pool1(spk1.reshape(T*B, 64, 28, 28))  # (T·B,64,14,14)\n",
    "\n",
    "        # ========== Layer 2 ==========\n",
    "        z2 = self.conv2(p1).reshape(T, B, 128, 14, 14)\n",
    "        spk2, mem2 = [], self.lif2.init_leaky()\n",
    "\n",
    "        for t in range(T):\n",
    "            s, mem2 = self.lif2(z2[t], mem2)\n",
    "            spk2.append(s)\n",
    "\n",
    "        spk2 = torch.stack(spk2)               # (T,B,128,14,14)\n",
    "        p2 = self.pool1(spk2.reshape(T*B, 128, 14, 14))  # (T·B,128,7,7)\n",
    "\n",
    "        # ========== Layer 3 ==========\n",
    "        z3 = self.conv3(p2).reshape(T, B, 256, 7, 7)\n",
    "        spk3, mem3 = [], self.lif3.init_leaky()\n",
    "\n",
    "        for t in range(T):\n",
    "            s, mem3 = self.lif3(z3[t], mem3)\n",
    "            spk3.append(s)\n",
    "\n",
    "        spk3 = torch.stack(spk3)               # (T,B,256,7,7)\n",
    "\n",
    "        # ========== FC Layer ==========\n",
    "        flat = spk3.reshape(T, B, -1)          # (T,B,256*7*7)\n",
    "        zF = torch.einsum(\"tbi,oi->tbo\", flat, self.fc.weight)\n",
    "\n",
    "        spk_out, mem_out = [], self.lif_out.init_leaky()\n",
    "        for t in range(T):\n",
    "            s, mem_out = self.lif_out(zF[t], mem_out)\n",
    "            spk_out.append(s)\n",
    "\n",
    "        spk_out = torch.stack(spk_out)         # (T,B,num_classes)\n",
    "        return spk_out, None\n",
    "\n",
    "\n",
    "class MNIST_CNN_SNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_outputs=100,   # 10 neurons per class\n",
    "                 hidden1=512,\n",
    "                 hidden2=256):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, padding=0)\n",
    "\n",
    "        # If batchnorm allowed (recommended):\n",
    "        # self.bn1 = nn.BatchNorm2d(16)\n",
    "        # self.bn2 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # After two conv + pool layers:\n",
    "        # 28x28 → 24x24 → pool → 12x12\n",
    "        # 12x12 → 8x8 → pool → 4x4\n",
    "        # channels=32 → 32*4*4 = 512 features\n",
    "\n",
    "        self.flat_features = 32 * 4 * 4\n",
    "\n",
    "        # ----------------------\n",
    "        # Fully-connected SNN\n",
    "        # ----------------------\n",
    "        self.fc1 = WeightDropoutLinear(self.flat_features, hidden1, bias=False)\n",
    "        self.fc2 = WeightDropoutLinear(hidden1,        hidden2, bias=False)\n",
    "        self.fc3 = WeightDropoutLinear(hidden2,        num_outputs, bias=False)\n",
    "\n",
    "        self.lif1 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "        self.lif2 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "        self.lif3 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        mean = (modelConstants.max_weight + modelConstants.min_weight) / 2.0\n",
    "        std  = (modelConstants.max_weight - modelConstants.min_weight) / 8.0\n",
    "\n",
    "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
    "            nn.init.normal_(layer.linear.weight, mean=mean, std=std)\n",
    "            with torch.no_grad():\n",
    "                layer.linear.weight.clamp_(modelConstants.min_weight, modelConstants.max_weight)\n",
    "\n",
    "    def _noisy_weight(self, w):\n",
    "        if self.training and modelConstants.training_noise_ctt:\n",
    "            if modelConstants.retention_noise:\n",
    "                w = add_retention_noise(w, modelConstants.std_ret_high, modelConstants.std_ret_low)\n",
    "            if modelConstants.quantization_noise:\n",
    "                w = apply_quant_noise(w)\n",
    "        return w\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [T, B, 1, 28, 28]\n",
    "        Output:\n",
    "            spk_rec_3: [T, B, num_outputs]\n",
    "            mem_rec_3: [T, B, num_outputs]\n",
    "        \"\"\"\n",
    "\n",
    "        T, B = x.shape[0], x.shape[1]\n",
    "\n",
    "        # ----------------------\n",
    "        # Initialize SNN states\n",
    "        # ----------------------\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # noise for linear layers\n",
    "        self.nW1 = torch.normal(torch.zeros_like(self.fc1.linear.weight), modelConstants.std_ctt)\n",
    "        self.nW2 = torch.normal(torch.zeros_like(self.fc2.linear.weight), modelConstants.std_ctt)\n",
    "        self.nW3 = torch.normal(torch.zeros_like(self.fc3.linear.weight), modelConstants.std_ctt)\n",
    "\n",
    "        w1 = self._noisy_weight(self.fc1.linear.weight)\n",
    "        w2 = self._noisy_weight(self.fc2.linear.weight)\n",
    "        w3 = self._noisy_weight(self.fc3.linear.weight)\n",
    "\n",
    "        spk_out, mem_out = [], []\n",
    "\n",
    "        # ----------------------\n",
    "        # Temporal loop\n",
    "        # ----------------------\n",
    "        for step in range(T):\n",
    "            img = x[step]        # [B, 1, 28, 28]\n",
    "\n",
    "            # ---- CNN feature extractor ----\n",
    "            out = self.conv1(img)         # [B, 16, 24, 24]\n",
    "            out = torch.relu(out)\n",
    "            out = self.pool(out)          # [B, 16, 12, 12]\n",
    "\n",
    "            out = self.conv2(out)         # [B, 32, 8, 8]\n",
    "            out = torch.relu(out)\n",
    "            out = self.pool(out)          # [B, 32, 4, 4]\n",
    "\n",
    "            out = out.view(B, -1)         # [B, 512]\n",
    "\n",
    "            # ---- SNN Classifier ----\n",
    "            # Layer 1\n",
    "            cur1 = Square.apply(out, w1)\n",
    "            with torch.no_grad():\n",
    "                noise1 = nn.functional.linear(out, self.nW1)\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur1 = cur1 + noise1\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            # Layer 2\n",
    "            cur2 = Square.apply(spk1, w2)\n",
    "            with torch.no_grad():\n",
    "                noise2 = nn.functional.linear(spk1, self.nW2)\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur2 = cur2 + noise2\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            # Output layer\n",
    "            cur3 = Square.apply(spk2, w3)\n",
    "            with torch.no_grad():\n",
    "                noise3 = nn.functional.linear(spk2, self.nW3)\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur3 = cur3 + noise3\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk_out.append(spk3)\n",
    "            mem_out.append(mem3)\n",
    "\n",
    "        return torch.stack(spk_out, dim=0), torch.stack(mem_out, dim=0)\n",
    "\n",
    "class NNfullyConnected(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_inputs=28*28,\n",
    "                 h1=512,\n",
    "                 h2=256,\n",
    "                 num_outputs=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_inputs  = num_inputs\n",
    "        self.h1          = h1\n",
    "        self.h2          = h2\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "        # 784 -> 512 -> 256 -> 10\n",
    "        self.fc1 = WeightDropoutLinear(self.num_inputs,  self.h1,          bias=False)\n",
    "        self.fc2 = WeightDropoutLinear(self.h1,          self.h2,          bias=False)\n",
    "        self.fc3 = WeightDropoutLinear(self.h2,          self.num_outputs, bias=False)\n",
    "\n",
    "        self.lif1 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "        self.lif2 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "        self.lif3 = digLIF(beta=1.0, reset_mechanism=\"zero\")\n",
    "\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        # keep centered in your tiny range\n",
    "        mean = (modelConstants.max_weight + modelConstants.min_weight) / 2.0\n",
    "        std  = (modelConstants.max_weight - modelConstants.min_weight) / 8.0\n",
    "\n",
    "        for layer in [self.fc1, self.fc2, self.fc3]:\n",
    "            nn.init.normal_(layer.linear.weight, mean=mean, std=std)\n",
    "            with torch.no_grad():\n",
    "                layer.linear.weight.clamp_(modelConstants.min_weight, modelConstants.max_weight)\n",
    "\n",
    "    def _noisy_weight(self, base_weight):\n",
    "        w = base_weight\n",
    "        if self.training and modelConstants.training_noise_ctt:\n",
    "            if modelConstants.retention_noise:\n",
    "                w = add_retention_noise(w, modelConstants.std_ret_high, modelConstants.std_ret_low)\n",
    "            if modelConstants.quantization_noise:\n",
    "                w = apply_quant_noise(w)\n",
    "        return w\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [T, B, 1, 28, 28] or [T, B, 28, 28] or [T, B, 784]\n",
    "\n",
    "        returns:\n",
    "            spk_rec_3: [T, B, 10]\n",
    "            mem_rec_3: [T, B, 10]\n",
    "        \"\"\"\n",
    "\n",
    "        T = x.shape[0]\n",
    "\n",
    "        # Flatten spatial dims\n",
    "        if x.dim() == 5:          # [T, B, C, H, W]\n",
    "            x_flat = x.view(T, x.shape[1], -1)\n",
    "        elif x.dim() == 4:        # [T, B, H, W]\n",
    "            x_flat = x.view(T, x.shape[1], -1)\n",
    "        elif x.dim() == 3:        # [T, B, F]\n",
    "            x_flat = x\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape {x.shape}\")\n",
    "\n",
    "        # init membrane\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        # sample weight noise for this forward\n",
    "        self.nW1 = torch.normal(torch.zeros_like(self.fc1.linear.weight), modelConstants.std_ctt)\n",
    "        self.nW2 = torch.normal(torch.zeros_like(self.fc2.linear.weight), modelConstants.std_ctt)\n",
    "        self.nW3 = torch.normal(torch.zeros_like(self.fc3.linear.weight), modelConstants.std_ctt)\n",
    "\n",
    "        # effective weights\n",
    "        noisy_w1 = self._noisy_weight(self.fc1.linear.weight)\n",
    "        noisy_w2 = self._noisy_weight(self.fc2.linear.weight)\n",
    "        noisy_w3 = self._noisy_weight(self.fc3.linear.weight)\n",
    "\n",
    "        spk_rec_3, mem_rec_3 = [], []\n",
    "\n",
    "        for step in range(T):\n",
    "            spk0 = x_flat[step]                      # [B, 784]\n",
    "\n",
    "            # layer 1\n",
    "            cur1 = Square.apply(spk0, noisy_w1)\n",
    "            with torch.no_grad():\n",
    "                noise1 = nn.functional.linear(spk0, self.nW1)\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur1 = cur1 + noise1\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "\n",
    "            # layer 2\n",
    "            cur2 = Square.apply(spk1, noisy_w2)\n",
    "            with torch.no_grad():\n",
    "                noise2 = nn.functional.linear(spk1, self.nW2)\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur2 = cur2 + noise2\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            # layer 3 (output)\n",
    "            cur3 = Square.apply(spk2, noisy_w3)\n",
    "            with torch.no_grad():\n",
    "                noise3 = nn.functional.linear(spk2, self.nW3)\n",
    "            if self.training and modelConstants.general_noise:\n",
    "                cur3 = cur3 + noise3\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk_rec_3.append(spk3)\n",
    "            mem_rec_3.append(mem3)\n",
    "\n",
    "        spk_rec_3 = torch.stack(spk_rec_3, dim=0)   # [T, B, 10]\n",
    "        mem_rec_3 = torch.stack(mem_rec_3, dim=0)   # [T, B, 10]\n",
    "\n",
    "        return spk_rec_3, mem_rec_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e34b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MNIST_CNN_SNN().to(device)\n",
    "with torch.no_grad():\n",
    "    for layer in [net.fc1, net.fc2, net.fc3]:\n",
    "        layer.linear.weight.copy_(\n",
    "            torch.rand_like(layer.linear.weight) * (modelConstants.max_weight - modelConstants.min_weight) + modelConstants.min_weight\n",
    "        )\n",
    "\n",
    "\n",
    "for lif in [net.lif1, net.lif2, net.lif3]:\n",
    "    with torch.no_grad():\n",
    "        lif.threshold.copy_((lif.threshold * modelConstants.Threshold_voltage).to(device))\n",
    "\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=2,          # first cycle length\n",
    "    T_mult=1,       # cycle length stays constant\n",
    "    eta_min=1e-5    # small floor, not zero\n",
    ")\n",
    "ce = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba7e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "counter = 0\n",
    "\n",
    "def train_and_evaluate(loss_fn = ce, num_epochs = 5, optimizer = optimizer, scheduler=scheduler, train_loader=train_loader, test_loader=test_loader):\n",
    "    epoch_times = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_batch = iter(train_loader)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            data = spikegen.latency(data, num_steps=modelConstants.num_steps, threshold=0.01, clip=True, first_spike_time=0, linear=True, normalize=True).cumsum(0)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            net.train()\n",
    "            spk_rec, mem_rec = net(data)\n",
    "\n",
    "            T = spk_rec.shape[0]\n",
    "\n",
    "            loss_val = torch.zeros((1,), dtype=dtype, device=device)\n",
    "            repeated_targets = targets.float().unsqueeze(0).repeat(T, 1).unsqueeze(-1)\n",
    "            logits = spk_rec.sum(dim=0)\n",
    "            loss_val = loss_fn(logits, targets)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for layer in [net.fc1, net.fc2, net.fc3]:\n",
    "                    layer.linear.weight.clamp_(0, modelConstants.max_weight)\n",
    "\n",
    "        end_time = time.time()\n",
    "        epoch_times.append(end_time - start_time)\n",
    "        print(\"epoch loss: \", loss_val)\n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            total = 0\n",
    "            correct = 0\n",
    "\n",
    "            for data, targets in test_loader:\n",
    "                data = data.to(device)\n",
    "                data = spikegen.latency(data, num_steps=modelConstants.num_steps, threshold=0.01, clip=True, first_spike_time=0, linear=True, normalize=True).cumsum(0)\n",
    "                targets = targets.to(device)\n",
    "                spk_rec, _ = net(data)\n",
    "                T, B, _ = spk_rec.shape\n",
    "\n",
    "                logits = spk_rec.sum(dim=0)\n",
    "                predicted = logits.argmax(dim=-1)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "\n",
    "                total += targets.size(0)\n",
    "\n",
    "            print(f\"Total correctly classified test set images: {correct}/{total}\")\n",
    "            print(f\"Test Set Accuracy: {100 * correct / total:.2f}%\\n\")\n",
    "            for param_group in optimizer.param_groups:\n",
    "                current_lr = param_group['lr']\n",
    "                print(f\"Current learning rate: {current_lr}\")\n",
    "            '''with open(\"model_accuracy.txt\", \"a\") as f:\n",
    "                f.write(f\"{epoch}: {100 * correct / total:.2f}%\\n\")'''\n",
    "    print(\"average epoch time: \", np.mean(epoch_times))\n",
    "    return 100 *(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82f4d5",
   "metadata": {},
   "source": [
    "### Epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ad145",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "best_params = None\n",
    "worst_accuracy = 100 \n",
    "\n",
    "thr_values = [0.3]\n",
    "hyperparams = [{\"thr\": thr} for thr in thr_values]\n",
    "\n",
    "for params in hyperparams:\n",
    "    V_th = params[\"thr\"]\n",
    "    net.lif1.threshold.data = torch.tensor(V_th)\n",
    "    accuracy = train_and_evaluate(loss_fn=ce, num_epochs = 30, optimizer = optimizer, scheduler=scheduler)\n",
    "    print(f\"Accuracy with params {params}: {accuracy}\")\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "        best_weights = net.fc1.linear.weight.clone()\n",
    "    if accuracy < worst_accuracy:\n",
    "        worst_accuracy = accuracy\n",
    "        worst_params = params\n",
    "        \n",
    "    print(f\"Best parameters: {best_params} with accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39617448",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfa175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def optuna_objective(trial):\n",
    "    global net, optimizer, scheduler, num_steps\n",
    "\n",
    "    # ---------- Hyperparameter search space ----------\n",
    "    # hidden sizes (must fit your hardware limits)\n",
    "    h1 = trial.suggest_int(\"h1\", 512, 2048, step=256)\n",
    "    h2 = trial.suggest_int(\"h2\", 256, 2048, step=256)\n",
    "\n",
    "    # learning rate (log-scale)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
    "\n",
    "    # threshold scaling (relative to your current Threshold_voltage)\n",
    "    thr_scale = trial.suggest_float(\"thr_scale\", 0.2, 1.0)\n",
    "\n",
    "    # number of timesteps for latency encoding\n",
    "    num_steps = trial.suggest_int(\"num_steps\", 10, 40, step=10)\n",
    "\n",
    "    # number of epochs per trial (keep small for speed)\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 3, 7)\n",
    "\n",
    "    net = NNfullyConnected(\n",
    "        num_inputs=28*28,\n",
    "        h1=h1,\n",
    "        h2=h2,\n",
    "        num_outputs=modelConstants.num_classes,\n",
    "    ).to(device)\n",
    "\n",
    "    for lif in [net.lif1, net.lif2, net.lif3]:\n",
    "        lif.threshold = (lif.threshold * thr_scale * modelConstants.Threshold_voltage).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999))\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=2,          # first cycle length\n",
    "        T_mult=1,       # cycle length stays constant\n",
    "        eta_min=1e-5    # small floor, not zero\n",
    "    )\n",
    "    acc = train_and_evaluate(\n",
    "        loss_fn=ce,\n",
    "        num_epochs=num_epochs,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "\n",
    "    # Optuna will maximize this\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e912198",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(optuna_objective, n_trials=20)  # start with 20; increase if useful\n",
    "\n",
    "print(\"Best value (accuracy):\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in study.trials:\n",
    "    print(t.number, t.value, t.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24dc7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = study.best_params\n",
    "\n",
    "h1 = best[\"h1\"]\n",
    "h2 = best[\"h2\"]\n",
    "lr = best[\"lr\"]\n",
    "thr_scale = best[\"thr_scale\"]\n",
    "num_steps = best[\"num_steps\"]\n",
    "\n",
    "net = NNfullyConnected(\n",
    "    num_inputs=28*28,\n",
    "    h1=h1,\n",
    "    h2=h2,\n",
    "    num_outputs=modelConstants.num_classes,\n",
    ").to(device)\n",
    "\n",
    "for lif in [net.lif1, net.lif2, net.lif3]:\n",
    "    lif.threshold = (lif.threshold * thr_scale * modelConstants.Threshold_voltage).to(device)\n",
    "\n",
    "final_acc = train_and_evaluate(\n",
    "    loss_fn=ce,\n",
    "    num_epochs=15,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "\n",
    "print(\"Final accuracy with best params:\", final_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
