{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a8d5f03",
   "metadata": {},
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c02194b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autocast, GradScaler\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\__init__.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malexnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconvnext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdensenet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\alexnet.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_presets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageClassification\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _log_api_usage_once\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_model, Weights, WeightsEnum\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1130\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9968d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_weight, max_weight = -0.7, 0.7\n",
    "num_weight_levels = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d381a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc6957f",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  # gives [0,1]\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                         (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_set = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_tf)\n",
    "test_set  = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=test_tf)\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False,\n",
    "                          num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a82b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_input_scale(loader, device, batches=50, q=0.99, sample_per_batch=200_000):\n",
    "    vals = []\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        if i >= batches:\n",
    "            break\n",
    "        x = x.to(device, non_blocking=True)\n",
    "\n",
    "        v = x.abs().flatten()\n",
    "\n",
    "        # NEW: cap how many elements you keep from each batch\n",
    "        if v.numel() > sample_per_batch:\n",
    "            idx = torch.randint(v.numel(), (sample_per_batch,), device=device)\n",
    "            v = v[idx]\n",
    "\n",
    "        vals.append(v)\n",
    "\n",
    "    v = torch.cat(vals)\n",
    "    return torch.quantile(v, q).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SCALE_TRAIN = estimate_input_scale(train_loader, device=device, batches=50, q=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eb99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SCALE_TEST = estimate_input_scale(test_loader, device=device, batches=50, q=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c4a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_encode_func(x, input_scale=1.0, clamp01=False):\n",
    "    \"\"\"\n",
    "    x: (B,C,H,W), can be negative (after Normalize)\n",
    "    returns: (B,2*C,H,W) = [x_pos, x_neg] where x = x_pos - x_neg\n",
    "\n",
    "    If clamp01=True -> enforce duty-cycle limit [0,1] (hardware-like) but introduces saturation.\n",
    "    \"\"\"\n",
    "    x = x / float(input_scale)\n",
    "    x_pos = torch.clamp(x, min=0.0)\n",
    "    x_neg = torch.clamp(-x, min=0.0)\n",
    "    if clamp01:\n",
    "        x_pos = x_pos.clamp(0.0, 1.0)\n",
    "        x_neg = x_neg.clamp(0.0, 1.0)\n",
    "    return torch.cat([x_pos, x_neg], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f58af",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9090a11",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47465728",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def clamp_weights_(model: nn.Module, lo: float, hi: float, clamp_bias: bool = True):\n",
    "    \"\"\"\n",
    "    Clamp weights (and optionally bias) in-place for any module that has\n",
    "    Tensor attributes `weight` / `bias`. Leaves BatchNorm params alone.\n",
    "    \"\"\"\n",
    "    for name, m in model.named_modules():\n",
    "        # Skip BN entirely\n",
    "        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):\n",
    "            continue\n",
    "\n",
    "        w = getattr(m, \"weight\", None)\n",
    "        if torch.is_tensor(w):\n",
    "            w.clamp_(lo, hi)\n",
    "\n",
    "def maskW(w: torch.Tensor) -> torch.Tensor:\n",
    "    w = torch.clamp(w, min=min_weight, max=max_weight)\n",
    "    step = (max_weight - min_weight) / (num_weight_levels - 1)\n",
    "    return ((w - min_weight) / step).round() * step + min_weight\n",
    "\n",
    "def maskW_ste(w: torch.Tensor) -> torch.Tensor:\n",
    "    # Forward: quantized weights\n",
    "    w_q = maskW(w)\n",
    "    # Backward: pretend quantization was identity (straight-through estimator)\n",
    "    return w + (w_q - w).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cef8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k: float = 1.38e-23   # Boltzmann constant (J/K)\n",
    "q: float = 1.602176634e-19\n",
    "T_r: float = 300\n",
    "n: float = 1.5\n",
    "k_mu: float = 1.5\n",
    "k_vt: float = 1e-3 # Typical value for threshold voltage temperature dependence (in V/K)\n",
    "Tr = 300\n",
    "\n",
    "def thermal_voltage(T):\n",
    "    return k * T / q\n",
    "\n",
    "def Id_subthreshold(W, L, mu, Cox, Vth, VGS, VDS, T, n):\n",
    "    Vt = thermal_voltage(T)\n",
    "    Is0 = (W / L) * mu * Cox * (Vt**2) * np.exp(1.8)\n",
    "    return Is0 * np.exp((VGS - Vth) / (n * Vt)) * (1 - np.exp(-VDS / Vt))\n",
    "\n",
    "class SubthresholdPVTNoise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        params: dict with\n",
    "            W, L, mu, Cox, Vth, VGS, VDS, T, n\n",
    "        sigmas: dict with\n",
    "            W, L, mu, Cox, Vth, VDS (normal std)\n",
    "            dT (uniform half-range)\n",
    "        \"\"\"\n",
    "        #self.p = params\n",
    "        self.W   = 44e-9\n",
    "        self.L   = 22e-9\n",
    "        self.mu  = 0.03\n",
    "        self.Cox = 0.03\n",
    "        self.Vth = 0.35\n",
    "        self.VGS = 0.25\n",
    "        self.VDS = 0.8\n",
    "        self.T0  = 310.0\n",
    "        self.n   = 1.5\n",
    "        self.dId_dW_value = 0.0\n",
    "        self.dId_dL_value = 0.0\n",
    "        self.dId_dmu_value = 0.0\n",
    "        self.dId_dCox_value = 0.0\n",
    "        self.dId_dVth_value = 0.0\n",
    "        self.dId_dVDS_value = 0.0\n",
    "        self.dId_dT_value = 0.0\n",
    "        self.s = {\n",
    "            \"W\":   0.10 * self.W,\n",
    "            \"L\":   0.10 * self.L,\n",
    "            \"mu\":  0.10 * self.mu,\n",
    "            \"Cox\": 0.10 * self.Cox,\n",
    "            \"Vth\": 0.10 * self.Vth,\n",
    "            \"VDS\": 0.10 * self.VDS,\n",
    "            \"T_min\": 248,\n",
    "            \"T_max\": 398,\n",
    "        }\n",
    "\n",
    "    def dId_dW(self):\n",
    "        Vt = thermal_voltage(self.T0)\n",
    "        return (self.mu * self.Cox * Vt**2 / self.L) * np.exp(1.8) * \\\n",
    "            np.exp((self.VGS - self.Vth) / (self.n * Vt)) * (1 - np.exp(-self.VDS / Vt))\n",
    "\n",
    "    def dId_dmu(self):\n",
    "        Vt = thermal_voltage(self.T0)\n",
    "        return (self.W * self.Cox * Vt**2 / self.L) * np.exp(1.8) * \\\n",
    "            np.exp((self.VGS - self.Vth) / (self.n * Vt)) * (1 - np.exp(-self.VDS / Vt))\n",
    "\n",
    "    def dId_dCox(self):\n",
    "        Vt = thermal_voltage(self.T0)\n",
    "        return (self.W * self.mu * Vt**2 / self.L) * np.exp(1.8) * \\\n",
    "            np.exp((self.VGS - self.Vth) / (self.n * Vt)) * (1 - np.exp(-self.VDS / Vt))\n",
    "\n",
    "    def dId_dL(self):\n",
    "        Vt = thermal_voltage(self.T0)\n",
    "        return -(self.W * self.mu * self.Cox * Vt**2 / self.L**2) * np.exp(1.8) * \\\n",
    "                np.exp((self.VGS - self.Vth) / (self.n * Vt)) * (1 - np.exp(-self.VDS / Vt))\n",
    "    \n",
    "    def dId_dVDS(self):\n",
    "        Vt = thermal_voltage(self.T0)\n",
    "        Is0 = (self.W / self.L) * self.mu * self.Cox * (Vt**2) * np.exp(1.8)\n",
    "        return Is0 * np.exp((self.VGS - self.Vth) / (self.n * Vt)) * (np.exp(-self.VDS / Vt) / Vt)\n",
    "    \n",
    "    def dId_dVth(self):\n",
    "        Id = Id_subthreshold(self.W, self.L, self.mu, self.Cox, self.Vth, self.VGS, self.VDS, self.T0, self.n)\n",
    "        Vt = thermal_voltage(self.T0)\n",
    "        return (-1 / (self.n * Vt)) * Id * np.exp((self.VGS - self.Vth) / (self.n * Vt)) * \\\n",
    "            (1 - np.exp(-self.VDS / Vt))\n",
    "\n",
    "    def dId_dT(self):\n",
    "        Vt = thermal_voltage(self.T0)\n",
    "        Id = Id_subthreshold(self.W, self.L, self.mu, self.Cox, self.Vth, self.VGS, self.VDS, self.T0, self.n)\n",
    "\n",
    "        term1 = k_mu - (( k_vt * self.T0) / (self.n * Vt)) * Id\n",
    "        term2 = (1 - np.exp(-self.VDS / Vt)) * Tr**(-k_mu) * self.T0**(k_mu - 1)\n",
    "        term3 = np.exp((self.VGS - self.Vth * Tr - k_vt * (self.T0-Tr)) / (self.n * Vt))\n",
    "        return Id * (term1 + term2 + term3)\n",
    "\n",
    "    def update_params(self):\n",
    "        self.dId_dW_value = self.dId_dW()\n",
    "        self.dId_dL_value = self.dId_dL()\n",
    "        self.dId_dmu_value = self.dId_dmu()\n",
    "        self.dId_dCox_value = self.dId_dCox()\n",
    "        self.dId_dVth_value = self.dId_dVth()\n",
    "        self.dId_dVDS_value = self.dId_dVDS()\n",
    "        self.dId_dT_value = self.dId_dT()\n",
    "\n",
    "    def forward(self, Id0):\n",
    "        \"\"\"\n",
    "        Id0: torch.Tensor (any shape)\n",
    "        returns: noisy Id tensor (same shape)\n",
    "        \"\"\"\n",
    "        device, dtype = Id0.device, Id0.dtype\n",
    "        self.update_params()\n",
    "\n",
    "        # ---- sample parameter variations ----\n",
    "        dW   = torch.normal(0.0, self.s[\"W\"],   size=Id0.shape, device=device, dtype=dtype)\n",
    "        dL   = torch.normal(0.0, self.s[\"L\"],   size=Id0.shape, device=device, dtype=dtype)\n",
    "        dmu  = torch.normal(0.0, self.s[\"mu\"],  size=Id0.shape, device=device, dtype=dtype)\n",
    "        dCox = torch.normal(0.0, self.s[\"Cox\"], size=Id0.shape, device=device, dtype=dtype)\n",
    "        dVth = torch.normal(0.0, self.s[\"Vth\"], size=Id0.shape, device=device, dtype=dtype)\n",
    "        dVDS = torch.normal(0.0, self.s[\"VDS\"], size=Id0.shape, device=device, dtype=dtype)\n",
    "        dT   = torch.empty_like(Id0).uniform_(self.s[\"T_min\"], self.s[\"T_max\"])\n",
    "        dT  = dT - self.T0\n",
    "\n",
    "        # dW = 0\n",
    "        # dL = 0\n",
    "        # dmu = 0\n",
    "        # dCox = 0    \n",
    "        # dVth = 0\n",
    "        # dVDS = 0\n",
    "        # dT = 0\n",
    "\n",
    "        # ---- analytical partial derivatives ----\n",
    "        dId = (\n",
    "            self.dId_dW_value           * dW   +\n",
    "            self.dId_dL_value           * dL   +\n",
    "            self.dId_dmu_value          * dmu  +\n",
    "            self.dId_dCox_value         * dCox +\n",
    "            self.dId_dVth_value         * dVth +\n",
    "            self.dId_dVDS_value * dVDS +\n",
    "            self.dId_dT_value * dT\n",
    "        )\n",
    "\n",
    "        return Id0 + dId\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceca718",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6088ae4",
   "metadata": {},
   "source": [
    "### Conv Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b4382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QATConv2d_no_other_noise(nn.Conv2d):\n",
    "    def forward(self, x):\n",
    "        w = maskW_ste(self.weight) if self.training else maskW(self.weight)\n",
    "        return F.conv2d(x, w, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "class QATLinear_no_other_noise(nn.Linear):\n",
    "    def forward(self, x):\n",
    "        w = maskW_ste(self.weight) if self.training else maskW(self.weight)\n",
    "        return F.linear(x, w, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11165813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class QATConv2d(nn.Conv2d):\n",
    "#     def __init__(self, *args, noise=None, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         # Register as a submodule so it moves to GPU + appears in state_dict\n",
    "#         self.noise = noise if noise is not None else SubthresholdPVTNoise()\n",
    "#         self.noise.update_params()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         w = maskW_ste(self.weight) if self.training else maskW(self.weight)\n",
    "\n",
    "#         # Add noise in forward pass (typically only during training)\n",
    "#         if self.training:\n",
    "#             w = self.noise(w)   # calls SubthresholdPVTNoise.forward(w)\n",
    "\n",
    "#         return F.conv2d(x, w, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
    "\n",
    "\n",
    "# class QATLinear(nn.Linear):\n",
    "#     def __init__(self, *args, noise=None, **kwargs):\n",
    "#         super().__init__(*args, **kwargs)\n",
    "#         self.noise = noise if noise is not None else SubthresholdPVTNoise()\n",
    "#         self.noise.update_params()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         w = maskW_ste(self.weight) if self.training else maskW(self.weight)\n",
    "#         if self.training:\n",
    "#             w = self.noise(w)\n",
    "#         return F.linear(x, w, self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08fe47",
   "metadata": {},
   "source": [
    "### neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32368433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftLIFRate(nn.Module):\n",
    "    def __init__(self, theta=0.0, gain=1.0, tau_rc=0.02, tau_ref=0.002, eps=1e-4):\n",
    "        super().__init__()\n",
    "        self.theta = nn.Parameter(torch.tensor(float(theta)))\n",
    "        self.gain  = nn.Parameter(torch.tensor(float(gain)))\n",
    "        self.tau_rc  = float(tau_rc)\n",
    "        self.tau_ref = float(tau_ref)\n",
    "        self.eps = float(eps)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Force the critical math to FP32 to avoid AMP/FP16 inf->nan gradients\n",
    "        z32 = z.float()\n",
    "        u = (self.gain.float()) * (z32 - self.theta.float())\n",
    "\n",
    "        # J = 1 + softplus(u) >= 1\n",
    "        Jm1 = F.softplus(u)                       # this is (J-1)\n",
    "        Jm1 = torch.clamp(Jm1, min=self.eps)      # prevents divide-by-0 / inf grads\n",
    "\n",
    "        # log(1 + 1/(J-1))\n",
    "        denom = self.tau_ref + self.tau_rc * torch.log1p(1.0 / Jm1)\n",
    "        rate  = 1.0 / denom\n",
    "\n",
    "        r = rate * self.tau_ref                   # normalize to [0,1]\n",
    "        r = torch.clamp(r, 0.0, 1.0)\n",
    "\n",
    "        return r.to(dtype=z.dtype)                # restore original dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574763fe",
   "metadata": {},
   "source": [
    "### ResNet blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b73b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_ch, out_ch, stride=1, act = None):\n",
    "        super().__init__()\n",
    "        self.conv1 = QATConv2d_no_other_noise(in_ch, out_ch, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.act1 = act if act is not None else SoftLIFRate()\n",
    "        self.conv2 = QATConv2d_no_other_noise(out_ch, out_ch, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.shortcut = nn.Identity()\n",
    "        if stride != 1 or in_ch != out_ch:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "            )\n",
    "\n",
    "        self.act2 = act if act is not None else SoftLIFRate()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \"Neuron type = ReLU\": apply after summed currents + BN\n",
    "        out = self.act1(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = out + self.shortcut(x)\n",
    "        out = self.act2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetCIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=10, in_channels=64, diff_encode=False, layers=(2, 2, 2, 2), act_theta=0.0, act_gain=1.0, tau_rc=0.02, tau_ref=0.002):\n",
    "        super().__init__()\n",
    "\n",
    "        stem_in = 6 if diff_encode else 3\n",
    "        self.in_ch = in_channels\n",
    "\n",
    "        def make_act():\n",
    "            return SoftLIFRate(theta=act_theta, gain=act_gain, tau_rc=tau_rc, tau_ref=tau_ref)\n",
    "\n",
    "        self.act_out = make_act()\n",
    "        \n",
    "        self.stem = nn.Sequential(\n",
    "            QATConv2d_no_other_noise(stem_in, in_channels, 3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            make_act(),\n",
    "        )\n",
    "\n",
    "        b1, b2, b3, b4 = layers\n",
    "        # ResNet18 layers: [2,2,2,2] blocks with channel sizes [in_channels,128,256,512]\n",
    "        self.layer1 = self._make_layer(in_channels, b1, stride=1, make_act=make_act)\n",
    "        self.layer2 = self._make_layer(128, b2, stride=2, make_act=make_act)\n",
    "        self.layer3 = self._make_layer(256, b3, stride=2, make_act=make_act)\n",
    "        self.layer4 = self._make_layer(512, b4, stride=2, make_act=make_act)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = QATLinear_no_other_noise(512, num_classes, bias=True)\n",
    "\n",
    "    def _make_layer(self, out_ch, num_blocks, stride, make_act):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        blocks = []\n",
    "        for s in strides:\n",
    "            blocks.append(BasicBlock(self.in_ch, out_ch, stride=s, act=make_act()))\n",
    "            self.in_ch = out_ch\n",
    "        return nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.pool(x).flatten(1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ddfb61",
   "metadata": {},
   "source": [
    "## learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f63dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetCIFAR(num_classes=10, in_channels=64, diff_encode=True, layers=(1, 1, 1, 2)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD + momentum is the default strong baseline for CIFAR ResNets\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4, nesterov=True)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "\n",
    "use_amp = (device.type == \"cuda\")\n",
    "scaler = GradScaler(device=device, enabled=use_amp)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, diff_encode=True):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_sum = 0.0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        if diff_encode:\n",
    "            x = diff_encode_func(x, input_scale=INPUT_SCALE_TEST, clamp01=False)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss_sum += float(loss) * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += int((pred == y).sum())\n",
    "        total += y.numel()\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "def train_one_epoch(model, loader, diff_encode=True):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        if diff_encode:\n",
    "            x = diff_encode_func(x, input_scale=INPUT_SCALE_TRAIN, clamp01=False)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast(device_type=device.type, enabled=use_amp):\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        clamp_weights_(model, min_weight, max_weight)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += float(loss) * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += int((pred == y).sum())\n",
    "        total += y.numel()\n",
    "\n",
    "    return running_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eceb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200  # this is the usual CIFAR recipe; should exceed 80% well before the end\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e31301",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt =  torch.load(\"checkpoints/cifar10/epoch_71_acc_0.9431.pt\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "fp_loss, fp_acc = evaluate(model, test_loader, True)\n",
    "print(f\"FP32   test acc: {(fp_acc * 100):.2f}% | loss: {fp_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6fc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | train loss 1.6795 acc 36.36% | test loss 2.0001 acc 37.47% | best 37.47%\n",
      "Epoch   2 | train loss 1.1369 acc 59.05% | test loss 1.5113 acc 43.88% | best 43.88%\n",
      "Epoch   3 | train loss 0.8740 acc 69.19% | test loss 1.3209 acc 56.11% | best 56.11%\n",
      "Epoch   4 | train loss 0.7356 acc 74.43% | test loss 2.5614 acc 37.54% | best 56.11%\n",
      "Epoch   5 | train loss 0.6690 acc 76.88% | test loss 0.9293 acc 68.08% | best 68.08%\n",
      "Epoch   6 | train loss 0.6141 acc 78.57% | test loss 3.1037 acc 37.79% | best 68.08%\n",
      "Epoch   7 | train loss 0.5840 acc 80.01% | test loss 1.3563 acc 57.27% | best 68.08%\n",
      "Epoch   8 | train loss 0.5560 acc 80.89% | test loss 0.7251 acc 75.49% | best 75.49%\n",
      "Epoch   9 | train loss 0.5337 acc 81.71% | test loss 0.9112 acc 71.68% | best 75.49%\n",
      "Epoch  10 | train loss 0.5168 acc 82.24% | test loss 1.8989 acc 52.99% | best 75.49%\n",
      "Epoch  11 | train loss 0.5016 acc 82.92% | test loss 0.5686 acc 80.72% | best 80.72%\n",
      "Epoch  12 | train loss 0.4919 acc 83.09% | test loss 1.2428 acc 63.67% | best 80.72%\n",
      "Epoch  13 | train loss 0.4732 acc 83.82% | test loss 0.8831 acc 72.92% | best 80.72%\n",
      "Epoch  14 | train loss 0.4698 acc 84.03% | test loss 1.8600 acc 55.04% | best 80.72%\n",
      "Epoch  15 | train loss 0.4586 acc 84.13% | test loss 0.7271 acc 74.78% | best 80.72%\n",
      "Epoch  16 | train loss 0.4514 acc 84.57% | test loss 0.7645 acc 74.93% | best 80.72%\n",
      "Epoch  17 | train loss 0.4465 acc 84.66% | test loss 0.9303 acc 70.29% | best 80.72%\n",
      "Epoch  18 | train loss 0.4418 acc 84.89% | test loss 1.4836 acc 59.48% | best 80.72%\n",
      "Epoch  19 | train loss 0.4386 acc 84.98% | test loss 0.7713 acc 75.06% | best 80.72%\n",
      "Epoch  20 | train loss 0.4280 acc 85.36% | test loss 0.7324 acc 75.85% | best 80.72%\n",
      "Epoch  21 | train loss 0.4253 acc 85.47% | test loss 0.7029 acc 77.06% | best 80.72%\n",
      "Epoch  22 | train loss 0.4230 acc 85.46% | test loss 1.4541 acc 54.89% | best 80.72%\n",
      "Epoch  23 | train loss 0.4132 acc 85.96% | test loss 0.6057 acc 79.03% | best 80.72%\n",
      "Epoch  24 | train loss 0.4145 acc 85.91% | test loss 0.7482 acc 74.86% | best 80.72%\n",
      "Epoch  25 | train loss 0.4026 acc 86.36% | test loss 0.7836 acc 74.13% | best 80.72%\n",
      "Epoch  26 | train loss 0.4025 acc 86.30% | test loss 1.0703 acc 64.62% | best 80.72%\n",
      "Epoch  27 | train loss 0.3995 acc 86.39% | test loss 0.9108 acc 68.91% | best 80.72%\n",
      "Epoch  28 | train loss 0.3974 acc 86.62% | test loss 0.7702 acc 75.00% | best 80.72%\n",
      "Epoch  29 | train loss 0.3948 acc 86.64% | test loss 0.9895 acc 70.08% | best 80.72%\n",
      "Epoch  30 | train loss 0.3949 acc 86.40% | test loss 1.0066 acc 70.60% | best 80.72%\n",
      "Epoch  31 | train loss 0.3912 acc 86.68% | test loss 0.8364 acc 74.34% | best 80.72%\n",
      "Epoch  32 | train loss 0.3849 acc 87.09% | test loss 0.6630 acc 77.98% | best 80.72%\n",
      "Epoch  33 | train loss 0.3848 acc 87.00% | test loss 1.2990 acc 61.43% | best 80.72%\n",
      "Epoch  34 | train loss 0.3816 acc 86.99% | test loss 1.9929 acc 44.82% | best 80.72%\n",
      "Epoch  35 | train loss 0.3773 acc 87.16% | test loss 1.0224 acc 70.59% | best 80.72%\n",
      "Epoch  36 | train loss 0.3773 acc 87.08% | test loss 0.8175 acc 73.43% | best 80.72%\n",
      "Epoch  37 | train loss 0.3730 acc 87.41% | test loss 0.6175 acc 79.03% | best 80.72%\n",
      "Epoch  38 | train loss 0.3717 acc 87.30% | test loss 0.9159 acc 68.42% | best 80.72%\n",
      "Epoch  39 | train loss 0.3724 acc 87.39% | test loss 0.8339 acc 74.31% | best 80.72%\n",
      "Epoch  40 | train loss 0.3658 acc 87.50% | test loss 0.7687 acc 75.65% | best 80.72%\n",
      "Epoch  41 | train loss 0.3662 acc 87.46% | test loss 0.7047 acc 77.68% | best 80.72%\n",
      "Epoch  42 | train loss 0.3607 acc 87.68% | test loss 0.7354 acc 76.22% | best 80.72%\n",
      "Epoch  43 | train loss 0.3596 acc 87.87% | test loss 0.7874 acc 75.29% | best 80.72%\n",
      "Epoch  44 | train loss 0.3565 acc 87.76% | test loss 0.4847 acc 83.54% | best 83.54%\n",
      "Epoch  45 | train loss 0.3541 acc 87.88% | test loss 1.0175 acc 66.05% | best 83.54%\n",
      "Epoch  46 | train loss 0.3557 acc 87.73% | test loss 1.1032 acc 68.13% | best 83.54%\n",
      "Epoch  47 | train loss 0.3544 acc 87.97% | test loss 0.6425 acc 78.65% | best 83.54%\n",
      "Epoch  48 | train loss 0.3501 acc 88.04% | test loss 1.1063 acc 67.82% | best 83.54%\n",
      "Epoch  49 | train loss 0.3475 acc 88.29% | test loss 0.5657 acc 80.45% | best 83.54%\n",
      "Epoch  50 | train loss 0.3498 acc 88.02% | test loss 0.7148 acc 77.06% | best 83.54%\n",
      "Epoch  51 | train loss 0.3426 acc 88.28% | test loss 1.0585 acc 66.91% | best 83.54%\n",
      "Epoch  52 | train loss 0.3477 acc 88.17% | test loss 0.9000 acc 70.36% | best 83.54%\n",
      "Epoch  53 | train loss 0.3421 acc 88.28% | test loss 0.7805 acc 72.90% | best 83.54%\n",
      "Epoch  54 | train loss 0.3460 acc 88.21% | test loss 1.1931 acc 65.47% | best 83.54%\n",
      "Epoch  55 | train loss 0.3401 acc 88.54% | test loss 0.9549 acc 70.55% | best 83.54%\n",
      "Epoch  56 | train loss 0.3379 acc 88.40% | test loss 0.6010 acc 80.74% | best 83.54%\n",
      "Epoch  57 | train loss 0.3372 acc 88.64% | test loss 0.9053 acc 72.99% | best 83.54%\n",
      "Epoch  58 | train loss 0.3331 acc 88.66% | test loss 0.5368 acc 81.44% | best 83.54%\n",
      "Epoch  59 | train loss 0.3292 acc 88.88% | test loss 0.9019 acc 73.69% | best 83.54%\n",
      "Epoch  60 | train loss 0.3312 acc 88.70% | test loss 0.6284 acc 79.22% | best 83.54%\n",
      "Epoch  61 | train loss 0.3247 acc 88.90% | test loss 0.7867 acc 74.88% | best 83.54%\n",
      "Epoch  62 | train loss 0.3305 acc 88.76% | test loss 1.1003 acc 69.99% | best 83.54%\n",
      "Epoch  63 | train loss 0.3224 acc 89.06% | test loss 1.1753 acc 66.92% | best 83.54%\n",
      "Epoch  64 | train loss 0.3239 acc 88.99% | test loss 1.2759 acc 64.07% | best 83.54%\n",
      "Epoch  65 | train loss 0.3166 acc 89.22% | test loss 0.7309 acc 76.41% | best 83.54%\n",
      "Epoch  66 | train loss 0.3168 acc 89.26% | test loss 1.1844 acc 64.40% | best 83.54%\n",
      "Epoch  67 | train loss 0.3158 acc 89.35% | test loss 0.8230 acc 73.75% | best 83.54%\n",
      "Epoch  68 | train loss 0.3131 acc 89.37% | test loss 0.8122 acc 74.32% | best 83.54%\n",
      "Epoch  69 | train loss 0.3073 acc 89.35% | test loss 0.6987 acc 77.94% | best 83.54%\n",
      "Epoch  70 | train loss 0.3082 acc 89.46% | test loss 0.6212 acc 81.47% | best 83.54%\n",
      "Epoch  71 | train loss 0.3105 acc 89.33% | test loss 0.4944 acc 83.72% | best 83.72%\n",
      "Epoch  72 | train loss 0.3037 acc 89.51% | test loss 0.5695 acc 81.52% | best 83.72%\n",
      "Epoch  73 | train loss 0.3100 acc 89.30% | test loss 1.5048 acc 59.11% | best 83.72%\n",
      "Epoch  74 | train loss 0.3052 acc 89.75% | test loss 0.6871 acc 77.17% | best 83.72%\n",
      "Epoch  75 | train loss 0.3010 acc 89.86% | test loss 0.5677 acc 81.37% | best 83.72%\n",
      "Epoch  76 | train loss 0.2970 acc 89.97% | test loss 0.6525 acc 78.76% | best 83.72%\n",
      "Epoch  77 | train loss 0.2998 acc 89.79% | test loss 0.6607 acc 78.13% | best 83.72%\n",
      "Epoch  78 | train loss 0.2986 acc 89.91% | test loss 0.5745 acc 81.88% | best 83.72%\n",
      "Epoch  79 | train loss 0.2920 acc 90.02% | test loss 0.5906 acc 79.63% | best 83.72%\n",
      "Epoch  80 | train loss 0.2912 acc 90.16% | test loss 0.7817 acc 75.39% | best 83.72%\n",
      "Epoch  81 | train loss 0.2886 acc 90.09% | test loss 0.5250 acc 83.38% | best 83.72%\n",
      "Epoch  82 | train loss 0.2866 acc 90.30% | test loss 0.4992 acc 83.86% | best 83.86%\n",
      "Epoch  83 | train loss 0.2824 acc 90.35% | test loss 0.5464 acc 81.62% | best 83.86%\n",
      "Epoch  84 | train loss 0.2815 acc 90.41% | test loss 0.4878 acc 83.25% | best 83.86%\n",
      "Epoch  85 | train loss 0.2818 acc 90.49% | test loss 0.6615 acc 77.28% | best 83.86%\n",
      "Epoch  86 | train loss 0.2750 acc 90.70% | test loss 0.5710 acc 81.19% | best 83.86%\n",
      "Epoch  87 | train loss 0.2771 acc 90.47% | test loss 0.4947 acc 83.43% | best 83.86%\n",
      "Epoch  88 | train loss 0.2754 acc 90.62% | test loss 0.7430 acc 75.90% | best 83.86%\n",
      "Epoch  89 | train loss 0.2702 acc 90.78% | test loss 0.5384 acc 82.65% | best 83.86%\n",
      "Epoch  90 | train loss 0.2661 acc 90.99% | test loss 1.0130 acc 70.88% | best 83.86%\n",
      "Epoch  91 | train loss 0.2712 acc 90.68% | test loss 0.7071 acc 79.03% | best 83.86%\n",
      "Epoch  92 | train loss 0.2635 acc 91.08% | test loss 0.5123 acc 83.51% | best 83.86%\n",
      "Epoch  93 | train loss 0.2667 acc 90.98% | test loss 0.6152 acc 80.73% | best 83.86%\n",
      "Epoch  94 | train loss 0.2601 acc 91.21% | test loss 0.6463 acc 79.18% | best 83.86%\n",
      "Epoch  95 | train loss 0.2560 acc 91.27% | test loss 0.4473 acc 85.05% | best 85.05%\n",
      "Epoch  96 | train loss 0.2481 acc 91.49% | test loss 0.5616 acc 81.57% | best 85.05%\n",
      "Epoch  97 | train loss 0.2555 acc 91.33% | test loss 0.5614 acc 81.31% | best 85.05%\n",
      "Epoch  98 | train loss 0.2529 acc 91.45% | test loss 0.5516 acc 81.63% | best 85.05%\n",
      "Epoch  99 | train loss 0.2511 acc 91.43% | test loss 0.5595 acc 81.84% | best 85.05%\n",
      "Epoch 100 | train loss 0.2445 acc 91.63% | test loss 0.4568 acc 84.61% | best 85.05%\n",
      "Epoch 101 | train loss 0.2401 acc 91.82% | test loss 1.2928 acc 66.56% | best 85.05%\n",
      "Epoch 102 | train loss 0.2418 acc 91.77% | test loss 0.4690 acc 85.36% | best 85.36%\n",
      "Epoch 103 | train loss 0.2386 acc 91.83% | test loss 0.6429 acc 79.77% | best 85.36%\n",
      "Epoch 104 | train loss 0.2348 acc 92.06% | test loss 0.7064 acc 78.61% | best 85.36%\n",
      "Epoch 105 | train loss 0.2385 acc 91.93% | test loss 0.7195 acc 78.26% | best 85.36%\n",
      "Epoch 106 | train loss 0.2280 acc 92.41% | test loss 0.6493 acc 80.04% | best 85.36%\n",
      "Epoch 107 | train loss 0.2316 acc 92.12% | test loss 0.5927 acc 81.54% | best 85.36%\n",
      "Epoch 108 | train loss 0.2209 acc 92.58% | test loss 0.9213 acc 72.23% | best 85.36%\n",
      "Epoch 109 | train loss 0.2248 acc 92.32% | test loss 0.8341 acc 74.54% | best 85.36%\n",
      "Epoch 110 | train loss 0.2247 acc 92.38% | test loss 0.7798 acc 77.21% | best 85.36%\n",
      "Epoch 111 | train loss 0.2161 acc 92.78% | test loss 0.6668 acc 79.89% | best 85.36%\n",
      "Epoch 112 | train loss 0.2129 acc 92.89% | test loss 1.0106 acc 73.83% | best 85.36%\n",
      "Epoch 113 | train loss 0.2161 acc 92.60% | test loss 0.4145 acc 86.44% | best 86.44%\n",
      "Epoch 114 | train loss 0.2131 acc 92.69% | test loss 0.5428 acc 82.88% | best 86.44%\n",
      "Epoch 115 | train loss 0.2030 acc 93.16% | test loss 0.5944 acc 81.68% | best 86.44%\n",
      "Epoch 116 | train loss 0.2065 acc 92.93% | test loss 0.5889 acc 82.23% | best 86.44%\n",
      "Epoch 117 | train loss 0.2002 acc 93.28% | test loss 0.7462 acc 77.28% | best 86.44%\n",
      "Epoch 118 | train loss 0.1953 acc 93.37% | test loss 0.5259 acc 82.57% | best 86.44%\n",
      "Epoch 119 | train loss 0.1979 acc 93.28% | test loss 0.5347 acc 84.21% | best 86.44%\n",
      "Epoch 120 | train loss 0.1937 acc 93.47% | test loss 0.5225 acc 83.69% | best 86.44%\n",
      "Epoch 121 | train loss 0.1884 acc 93.64% | test loss 0.6486 acc 80.66% | best 86.44%\n",
      "Epoch 122 | train loss 0.1906 acc 93.59% | test loss 0.5441 acc 82.71% | best 86.44%\n",
      "Epoch 123 | train loss 0.1821 acc 93.96% | test loss 0.4889 acc 85.59% | best 86.44%\n",
      "Epoch 124 | train loss 0.1809 acc 93.75% | test loss 0.5128 acc 84.10% | best 86.44%\n",
      "Epoch 125 | train loss 0.1752 acc 94.08% | test loss 0.7968 acc 75.20% | best 86.44%\n",
      "Epoch 126 | train loss 0.1736 acc 94.16% | test loss 0.6419 acc 80.47% | best 86.44%\n",
      "Epoch 127 | train loss 0.1693 acc 94.27% | test loss 0.5295 acc 83.50% | best 86.44%\n",
      "Epoch 128 | train loss 0.1671 acc 94.38% | test loss 0.7301 acc 78.69% | best 86.44%\n",
      "Epoch 129 | train loss 0.1686 acc 94.33% | test loss 0.4927 acc 84.76% | best 86.44%\n",
      "Epoch 130 | train loss 0.1589 acc 94.58% | test loss 0.4062 acc 86.81% | best 86.81%\n",
      "Epoch 131 | train loss 0.1604 acc 94.68% | test loss 0.8870 acc 75.96% | best 86.81%\n",
      "Epoch 132 | train loss 0.1558 acc 94.73% | test loss 0.5308 acc 83.85% | best 86.81%\n",
      "Epoch 133 | train loss 0.1561 acc 94.64% | test loss 0.5258 acc 83.01% | best 86.81%\n",
      "Epoch 134 | train loss 0.1498 acc 94.87% | test loss 0.3942 acc 87.41% | best 87.41%\n",
      "Epoch 135 | train loss 0.1457 acc 95.03% | test loss 0.4714 acc 84.77% | best 87.41%\n",
      "Epoch 136 | train loss 0.1478 acc 94.96% | test loss 0.5607 acc 82.31% | best 87.41%\n",
      "Epoch 137 | train loss 0.1431 acc 95.17% | test loss 0.4856 acc 85.28% | best 87.41%\n",
      "Epoch 138 | train loss 0.1353 acc 95.59% | test loss 0.5435 acc 83.81% | best 87.41%\n",
      "Epoch 139 | train loss 0.1340 acc 95.49% | test loss 0.5540 acc 83.99% | best 87.41%\n",
      "Epoch 140 | train loss 0.1281 acc 95.74% | test loss 0.4076 acc 86.84% | best 87.41%\n",
      "Epoch 141 | train loss 0.1263 acc 95.70% | test loss 0.3547 acc 89.68% | best 89.68%\n",
      "Epoch 142 | train loss 0.1233 acc 95.94% | test loss 0.4462 acc 86.09% | best 89.68%\n",
      "Epoch 143 | train loss 0.1206 acc 95.95% | test loss 0.5827 acc 82.24% | best 89.68%\n",
      "Epoch 144 | train loss 0.1152 acc 96.17% | test loss 0.4497 acc 86.77% | best 89.68%\n",
      "Epoch 145 | train loss 0.1120 acc 96.27% | test loss 0.4654 acc 85.27% | best 89.68%\n",
      "Epoch 146 | train loss 0.1117 acc 96.25% | test loss 0.3648 acc 89.11% | best 89.68%\n",
      "Epoch 147 | train loss 0.1064 acc 96.48% | test loss 0.5256 acc 85.35% | best 89.68%\n",
      "Epoch 148 | train loss 0.1022 acc 96.58% | test loss 0.6348 acc 82.11% | best 89.68%\n",
      "Epoch 149 | train loss 0.1008 acc 96.71% | test loss 0.5563 acc 84.70% | best 89.68%\n",
      "Epoch 150 | train loss 0.1005 acc 96.65% | test loss 0.3874 acc 88.59% | best 89.68%\n",
      "Epoch 151 | train loss 0.0949 acc 96.82% | test loss 0.3824 acc 88.64% | best 89.68%\n",
      "Epoch 152 | train loss 0.0891 acc 97.11% | test loss 0.5808 acc 84.40% | best 89.68%\n",
      "Epoch 153 | train loss 0.0870 acc 97.19% | test loss 0.4246 acc 87.39% | best 89.68%\n",
      "Epoch 154 | train loss 0.0795 acc 97.47% | test loss 0.4302 acc 87.32% | best 89.68%\n",
      "Epoch 155 | train loss 0.0775 acc 97.49% | test loss 0.5271 acc 84.93% | best 89.68%\n",
      "Epoch 156 | train loss 0.0747 acc 97.70% | test loss 0.4761 acc 86.59% | best 89.68%\n",
      "Epoch 157 | train loss 0.0732 acc 97.66% | test loss 0.4501 acc 87.27% | best 89.68%\n",
      "Epoch 158 | train loss 0.0686 acc 97.82% | test loss 0.4945 acc 86.51% | best 89.68%\n",
      "Epoch 159 | train loss 0.0650 acc 97.90% | test loss 0.3482 acc 89.67% | best 89.68%\n",
      "Epoch 160 | train loss 0.0624 acc 98.05% | test loss 0.4595 acc 86.62% | best 89.68%\n",
      "Epoch 161 | train loss 0.0607 acc 98.10% | test loss 0.4261 acc 87.45% | best 89.68%\n",
      "Epoch 162 | train loss 0.0578 acc 98.16% | test loss 0.6294 acc 82.98% | best 89.68%\n",
      "Epoch 163 | train loss 0.0520 acc 98.44% | test loss 0.3971 acc 88.62% | best 89.68%\n",
      "Epoch 164 | train loss 0.0497 acc 98.53% | test loss 0.3599 acc 89.41% | best 89.68%\n",
      "Epoch 165 | train loss 0.0461 acc 98.64% | test loss 0.3143 acc 90.67% | best 90.67%\n",
      "Epoch 166 | train loss 0.0399 acc 98.87% | test loss 0.3202 acc 90.99% | best 90.99%\n",
      "Epoch 167 | train loss 0.0402 acc 98.80% | test loss 0.3750 acc 88.80% | best 90.99%\n",
      "Epoch 168 | train loss 0.0379 acc 98.87% | test loss 0.3083 acc 90.80% | best 90.99%\n",
      "Epoch 169 | train loss 0.0368 acc 98.95% | test loss 0.3876 acc 89.11% | best 90.99%\n",
      "Epoch 170 | train loss 0.0321 acc 99.13% | test loss 0.4490 acc 88.63% | best 90.99%\n",
      "Epoch 171 | train loss 0.0282 acc 99.27% | test loss 0.3713 acc 90.16% | best 90.99%\n",
      "Epoch 172 | train loss 0.0270 acc 99.31% | test loss 0.3866 acc 89.91% | best 90.99%\n",
      "Epoch 173 | train loss 0.0272 acc 99.30% | test loss 0.3080 acc 91.19% | best 91.19%\n",
      "Epoch 174 | train loss 0.0235 acc 99.46% | test loss 0.3194 acc 90.72% | best 91.19%\n",
      "Epoch 175 | train loss 0.0217 acc 99.51% | test loss 0.3008 acc 91.85% | best 91.85%\n",
      "Epoch 176 | train loss 0.0186 acc 99.62% | test loss 0.3839 acc 89.74% | best 91.85%\n",
      "Epoch 177 | train loss 0.0185 acc 99.61% | test loss 0.3070 acc 91.42% | best 91.85%\n",
      "Epoch 178 | train loss 0.0172 acc 99.69% | test loss 0.2722 acc 92.59% | best 92.59%\n",
      "Epoch 179 | train loss 0.0160 acc 99.70% | test loss 0.3065 acc 91.56% | best 92.59%\n",
      "Epoch 180 | train loss 0.0149 acc 99.73% | test loss 0.3054 acc 91.94% | best 92.59%\n",
      "Epoch 181 | train loss 0.0132 acc 99.79% | test loss 0.2636 acc 92.70% | best 92.70%\n",
      "Epoch 182 | train loss 0.0134 acc 99.78% | test loss 0.2911 acc 91.92% | best 92.70%\n",
      "Epoch 183 | train loss 0.0120 acc 99.83% | test loss 0.2509 acc 92.96% | best 92.96%\n",
      "Epoch 184 | train loss 0.0112 acc 99.86% | test loss 0.2415 acc 93.09% | best 93.09%\n",
      "Epoch 185 | train loss 0.0101 acc 99.89% | test loss 0.2388 acc 93.09% | best 93.09%\n",
      "Epoch 186 | train loss 0.0102 acc 99.88% | test loss 0.2467 acc 93.05% | best 93.09%\n",
      "Epoch 187 | train loss 0.0099 acc 99.89% | test loss 0.2301 acc 93.43% | best 93.43%\n",
      "Epoch 188 | train loss 0.0091 acc 99.91% | test loss 0.2416 acc 93.16% | best 93.43%\n",
      "Epoch 189 | train loss 0.0089 acc 99.91% | test loss 0.2382 acc 93.35% | best 93.43%\n",
      "Epoch 190 | train loss 0.0082 acc 99.94% | test loss 0.2221 acc 93.67% | best 93.67%\n",
      "Epoch 191 | train loss 0.0080 acc 99.93% | test loss 0.2250 acc 93.66% | best 93.67%\n",
      "Epoch 192 | train loss 0.0078 acc 99.94% | test loss 0.2243 acc 93.65% | best 93.67%\n",
      "Epoch 193 | train loss 0.0073 acc 99.97% | test loss 0.2181 acc 93.70% | best 93.70%\n",
      "Epoch 194 | train loss 0.0075 acc 99.95% | test loss 0.2164 acc 93.70% | best 93.70%\n",
      "Epoch 195 | train loss 0.0076 acc 99.94% | test loss 0.2229 acc 93.61% | best 93.70%\n",
      "Epoch 196 | train loss 0.0069 acc 99.97% | test loss 0.2184 acc 93.96% | best 93.96%\n",
      "Epoch 197 | train loss 0.0073 acc 99.95% | test loss 0.2185 acc 93.79% | best 93.96%\n",
      "Epoch 198 | train loss 0.0071 acc 99.96% | test loss 0.2165 acc 93.83% | best 93.96%\n",
      "Epoch 199 | train loss 0.0073 acc 99.96% | test loss 0.2152 acc 93.85% | best 93.96%\n",
      "Epoch 200 | train loss 0.0067 acc 99.97% | test loss 0.2177 acc 93.76% | best 93.96%\n",
      "Done. Best test accuracy: 93.96 %\n"
     ]
    }
   ],
   "source": [
    "for ep in range(1, epochs + 1):\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_loader)\n",
    "    te_loss, te_acc = evaluate(model, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "    best_acc = max(best_acc, te_acc)\n",
    "    print(f\"Epoch {ep:3d} | train loss {tr_loss:.4f} acc {tr_acc*100:5.2f}%\"\n",
    "            f\" | test loss {te_loss:.4f} acc {te_acc*100:5.2f}% | best {best_acc*100:5.2f}%\")\n",
    "    torch.save({\"state_dict\": model.state_dict(), \"test_acc\": te_acc}, f\"checkpoints/cifar10/epoch_{ep}_acc_{te_acc:.4f}.pt\")\n",
    "\n",
    "print(\"Done. Best test accuracy:\", best_acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f3a3a8",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee544ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP32   test acc: 93.76% | loss: 0.2177\n"
     ]
    }
   ],
   "source": [
    "fp_loss, fp_acc = evaluate(model, test_loader, True)\n",
    "print(f\"FP32   test acc: {(fp_acc * 100):.2f}% | loss: {fp_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b924781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt =  torch.load(\"\")\n",
    "# model_fp = ResNetCIFAR(num_classes=10, in_channels=64, diff_encode=True, layers=(1, 1, 1, 2)).to(device)\n",
    "# model_fp.load_state_dict(ckpt[\"state_dict\"], strict=True)\n",
    "# fp_loss, fp_acc = evaluate(model_fp, test_loader, True)\n",
    "# print(f\"FP32   test acc: {(fp_acc * 100):.2f}% | loss: {fp_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286026d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_named_weight_tensors(model: nn.Module, include_bias: bool = False):\n",
    "    # Only \"synaptic\" layers\n",
    "    for mod_name, mod in model.named_modules():\n",
    "        if isinstance(mod, (nn.Conv2d, nn.Linear)):\n",
    "            w = getattr(mod, \"weight\", None)\n",
    "            if w is not None:\n",
    "                yield f\"{mod_name}.weight\", w\n",
    "            if include_bias:\n",
    "                b = getattr(mod, \"bias\", None)\n",
    "                if b is not None:\n",
    "                    yield f\"{mod_name}.bias\", b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee6891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensors scanned: 15\n",
      "Quant bounds: min=-0.441908, max=0.504627\n"
     ]
    }
   ],
   "source": [
    "w_mins, w_maxs = [], []\n",
    "names = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, w in iter_named_weight_tensors(model, include_bias=False):\n",
    "        names.append(name)\n",
    "        w_mins.append(w.detach().min().item())\n",
    "        w_maxs.append(w.detach().max().item())\n",
    "\n",
    "print(\"tensors scanned:\", len(names))\n",
    "min_w = float(min(w_mins))\n",
    "max_w = float(max(w_maxs))\n",
    "\n",
    "print(f\"Quant bounds: min={min_w:.6g}, max={max_w:.6g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c170894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_qnoise(w: torch.Tensor, noise) -> torch.Tensor:\n",
    "    # noise.forward(w) should return same shape tensor\n",
    "    w_noisy = noise(w)\n",
    "    return torch.clamp(w_noisy, min=min_weight, max=max_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _functional_call(module: nn.Module, params: dict, buffers: dict, x: torch.Tensor):\n",
    "    # Preferred (PyTorch 2.x): torch.func.functional_call\n",
    "    from torch.func import functional_call\n",
    "\n",
    "    state = {**params, **buffers}\n",
    "    return functional_call(module, state, (x,))\n",
    "\n",
    "class DiffModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a base model, but runs forward with \"virtual\" modified weights:\n",
    "      mode = \"fp\" | \"quant\" | \"qnoise\"\n",
    "    \"\"\"\n",
    "    def __init__(self, base: nn.Module, mode: str, noise=None, include_bias: bool = True):\n",
    "        super().__init__()\n",
    "        assert mode in {\"fp\", \"quant\", \"qnoise\"}\n",
    "        self.base = base\n",
    "        self.mode = mode\n",
    "        self.noise = noise\n",
    "        self.include_bias = include_bias\n",
    "\n",
    "        # Cache target parameter names for Conv/Linear\n",
    "        self.target_param_names = set(name for name, _ in iter_named_weight_tensors(base, include_bias=include_bias))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Grab current params/buffers each call (safe even if base changes)\n",
    "        params = dict(self.base.named_parameters())\n",
    "        buffers = dict(self.base.named_buffers())\n",
    "\n",
    "        # Build overridden params dict\n",
    "        new_params = {}\n",
    "        for name, p in params.items():\n",
    "            if name in self.target_param_names:\n",
    "                w = p\n",
    "                if self.mode == \"quant\":\n",
    "                    w = maskW(w)\n",
    "                if self.mode == \"qnoise\":\n",
    "                    if self.noise is None:\n",
    "                        raise ValueError(\"mode='qnoise' requires a noise object\")\n",
    "                    w = apply_qnoise(w, self.noise)\n",
    "                new_params[name] = w\n",
    "            else:\n",
    "                new_params[name] = p\n",
    "\n",
    "        return _functional_call(self.base, new_params, buffers, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0b47ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "te_loss, te_acc = evaluate(model, test_loader, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04483de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93.76"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te_acc * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_fp = DiffModel(model, mode=\"fp\").to(device).eval()\n",
    "fp_loss, fp_acc= evaluate(diff_fp, test_loader, True)\n",
    "print(f\"FP32    test acc: {(fp_acc * 100):.2f}% | loss: {fp_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fd240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUANT   test acc: 70.74% | loss: 1.0473\n"
     ]
    }
   ],
   "source": [
    "diff_q = DiffModel(model, mode=\"quant\").to(device).eval()\n",
    "q_loss, q_acc = evaluate(diff_q, test_loader, True)\n",
    "print(f\"QUANT   test acc: {(q_acc * 100):.2f}% | loss: {q_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcda5258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q+NOISE test acc: 10.03% | loss: 2.7797\n"
     ]
    }
   ],
   "source": [
    "noise = SubthresholdPVTNoise()\n",
    "noise.update_params()\n",
    "diff_qn = DiffModel(model, mode=\"qnoise\", noise=noise).to(device).eval()\n",
    "qn_loss, qn_acc = evaluate(diff_qn, test_loader, True)\n",
    "print(f\"Q+NOISE test acc: {(qn_acc * 100):.2f}% | loss: {qn_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
